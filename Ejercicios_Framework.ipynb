{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejercicio 1. Clasificación de dígitos manuscritos\n",
        "\n",
        "    Framework: TensorFlow y PyTorch\n",
        "\n",
        "    Dataset: MNIST https://www.tensorflow.org/datasets/catalog/mnist\n",
        "\n",
        "    Construir una CNN para clasificar dígitos (0–9) a partir de imágenes 28x28.\n",
        "\n",
        "    Implementación: Redes convolucionales, normalización, precisión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Cargar datos\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizar imágenes\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# One-hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Construir modelo CNN\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluar\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Precisión en test: {test_acc:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transforma: normalización a [0,1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Cargar datos\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Modelo CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # 26x26 -> 13x13\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # 11x11 -> 5x5\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} completada\")\n",
        "\n",
        "# Evaluación\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        correct += (pred == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "print(f\"Precisión en test: {correct / total:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Obtener todas las predicciones y etiquetas verdaderas\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Crear matriz de confusión\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusión - MNIST CNN\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizar algunas predicciones incorrectas\n",
        "incorrect_images = []\n",
        "incorrect_labels = []\n",
        "incorrect_preds = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        for i in range(len(predicted)):\n",
        "            if predicted[i] != labels[i]:\n",
        "                incorrect_images.append(data[i])\n",
        "                incorrect_labels.append(labels[i])\n",
        "                incorrect_preds.append(predicted[i])\n",
        "            if len(incorrect_images) >= 9:\n",
        "                break\n",
        "        if len(incorrect_images) >= 9:\n",
        "            break\n",
        "\n",
        "# Mostrar imágenes\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(incorrect_images[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(f\"Pred: {incorrect_preds[i]}, Real: {incorrect_labels[i]}\")\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Ejemplos de Clasificaciones Incorrectas\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Clasificación de rayos X (neumonía vs sano)\n",
        "\n",
        "    Framework: Keras + TensorFlow\n",
        "\n",
        "    Dataset: Chest X-ray Pneumonia (Kaggle) https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "    Detectar neumonía en radiografías con una red convolucional.\n",
        "\n",
        "    Implementación: Aumento de datos, regularización, evaluación con recall y F1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import kagglehub\n",
        "\n",
        "# Descargar y guardar el dataset\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 2: Preprocesamiento y Aumento de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Rutas\n",
        "train_dir = f\"{path}/chest_xray/train\"\n",
        "val_dir = f\"{path}/chest_xray/val\"\n",
        "test_dir = f\"{path}/chest_xray/test\"\n",
        "\n",
        "# Aumento de datos para el entrenamiento\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 3: Crear modelo CNN con regularización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Paso 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=3,\n",
        "    validation_data=val_generator\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 5: Evaluación con Recall y F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predicciones\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_labels = (y_pred > 0.5).astype(int).flatten()\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Reporte completo\n",
        "print(classification_report(y_true, y_pred_labels, target_names=['Normal', 'Neumonía']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extras: Matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Normal', 'Neumonía'], yticklabels=['Normal', 'Neumonía'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title(\"Matriz de Confusión - Rayos X\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejercicio3_ . Análisis de sentimiento en reseñas de películas\n",
        "\n",
        "    Framework: TensorFlow o PyTorch\n",
        "\n",
        "    Dataset: IMDb Sentiment Analysis Dataset https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "    Objetivo: Clasificar reseñas como positivas o negativas usando LSTM.\n",
        "\n",
        "    Implementación: Procesamiento de texto, embeddings, redes recurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 1: Descargar y preparar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Cargar las top 10,000 palabras más frecuentes\n",
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Padding para que todas las reseñas tengan la misma longitud\n",
        "max_len = 200\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 2: Crear el modelo con Embeddings + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 3: Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 4: Evaluar el modelo\n",
        "python\n",
        "Copiar\n",
        "Editar\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Precisión en test: {accuracy:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 5: Probar con una reseña personalizada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Convertir texto a secuencia de IDs\n",
        "def encode_review(text):\n",
        "    tokens = text.lower().split()\n",
        "    encoded = [word_index.get(word, 2) for word in tokens]  # 2: \"unknown\"\n",
        "    return pad_sequences([encoded], maxlen=max_len)\n",
        "\n",
        "# Ejemplo\n",
        "sample = \"The movie was fantastic and full of suspense\"\n",
        "encoded_sample = encode_review(sample)\n",
        "prediction = model.predict(encoded_sample)[0][0]\n",
        "\n",
        "print(\"Sentimiento:\", \"Positivo 👍\" if prediction > 0.5 else \"Negativo 👎\", f\"({prediction:.4f})\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Código para visualizar las métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extraer métricas del historial\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# Precisión\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc, 'bo-', label='Entrenamiento')\n",
        "plt.plot(epochs, val_acc, 'ro-', label='Validación')\n",
        "plt.title('Precisión por época')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Precisión')\n",
        "plt.legend()\n",
        "\n",
        "# Pérdida\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'bo-', label='Entrenamiento')\n",
        "plt.plot(epochs, val_loss, 'ro-', label='Validación')\n",
        "plt.title('Pérdida por época')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " EJERCICIO 4 Predicción del precio de viviendas\n",
        "\n",
        "    Framework: Keras\n",
        "\n",
        "    Dataset: fetch_california_housing de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
        "\n",
        "    Predecir el valor de viviendas en California usando un MLP.\n",
        "\n",
        "    Implementación: Regresión, MSE, normalización de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Cargar datos\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Separar en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos (muy importante para MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)  # Salida escalar para regresión\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # MSE + MAE\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "print(\"Entrenamiento finalizado\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"MSE: {loss:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validación')\n",
        "plt.title('MSE por época')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Clasificación de flores por imagen\n",
        "\n",
        "    Framework: TensorFlow (transfer learning con MobileNet)\n",
        "\n",
        "    Dataset: Oxford Flowers 102 https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\n",
        "\n",
        "    Clasificar flores con redes preentrenadas (VGG, ResNet).\n",
        "\n",
        "    Implementación: Transfer learning, fine-tuning, freezing layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "preparar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Cargar el dataset Oxford Flowers 102\n",
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'oxford_flowers102',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Define tamaños\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Función de preprocesamiento\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Cargar dataset\n",
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'oxford_flowers102',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "# Aplicar la función y preparar los datasets\n",
        "train_ds = ds_train.map(format_image).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = ds_val.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = ds_test.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Cargar base preentrenada sin la cabeza final\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "base_model.trainable = False  # Freeze layers\n",
        "\n",
        "# Construir modelo final\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(102, activation='softmax')  # 102 clases de flores\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Descongelar últimas capas del modelo base\n",
        "base_model.trainable = True\n",
        "\n",
        "# Entrenar solo las últimas capas\n",
        "fine_tune_at = 100  # congela hasta esta capa\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # tasa de aprendizaje baja\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fase 2 de entrenamiento\n",
        "fine_tune_epochs = 5\n",
        "history_fine = model.fit(train_ds, validation_data=val_ds, epochs=fine_tune_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f\"Precisión final en test: {acc:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Obtener nombres de las flores desde la metadata del dataset\n",
        "class_names = ds_info.features['label'].names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Obtener un lote de imágenes del test set\n",
        "for images, labels in test_ds.take(1):\n",
        "    preds = model.predict(images)\n",
        "    pred_labels = tf.argmax(preds, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy())\n",
        "    plt.axis('off')\n",
        "    true_label = class_names[labels[i].numpy()]\n",
        "    predicted_label = class_names[pred_labels[i].numpy()]\n",
        "    color = 'green' if predicted_label == true_label else 'red'\n",
        "    plt.title(f\"Pred: {predicted_label}\\nReal: {true_label}\", color=color)\n",
        "plt.suptitle(\"Predicciones del modelo - Oxford Flowers 102\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}