{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cb9fca",
   "metadata": {},
   "source": [
    "# ğŸ“‹ Estructura del Proyecto - 5 Etapas\n",
    "\n",
    "Este notebook estÃ¡ organizado en 5 etapas claramente definidas para facilitar su comprensiÃ³n y ejecuciÃ³n:\n",
    "\n",
    "## ğŸ”§ Etapa 1: ConfiguraciÃ³n y Descarga de Datos\n",
    "- InstalaciÃ³n de dependencias necesarias (Transformers, PyTorch, etc.)\n",
    "- VerificaciÃ³n de GPU y configuraciÃ³n del dispositivo\n",
    "- Descarga automÃ¡tica de textos del Proyecto Gutenberg\n",
    "\n",
    "## ğŸ“Š Etapa 2: Preprocesamiento y AnÃ¡lisis Exploratorio\n",
    "- Limpieza y tokenizaciÃ³n de textos\n",
    "- AnÃ¡lisis del corpus y vocabulario\n",
    "- PreparaciÃ³n de secuencias para el modelo LSTM\n",
    "\n",
    "## ğŸ—ï¸ Etapa 3: Arquitectura del Modelo\n",
    "- Fine-tuning de GPT-2 para generaciÃ³n de texto\n",
    "- ConfiguraciÃ³n de tokenizador y modelo preentrenado\n",
    "- PreparaciÃ³n para entrenamiento con Transformers\n",
    "\n",
    "## ğŸš€ Etapa 4: Entrenamiento\n",
    "- Entrenamiento del modelo por 5 Ã©pocas\n",
    "- Monitoreo en tiempo real del progreso\n",
    "- Guardado del mejor modelo\n",
    "\n",
    "## ğŸ“ˆ Etapa 5: EvaluaciÃ³n y Resultados\n",
    "- GeneraciÃ³n de texto automÃ¡tico\n",
    "- AnÃ¡lisis de la calidad del texto generado\n",
    "- VisualizaciÃ³n de mÃ©tricas de rendimiento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747571",
   "metadata": {},
   "source": [
    "# GeneraciÃ³n de Texto AutomÃ¡tico - Project Gutenberg\n",
    "## Framework: Transformers (Sin TensorFlow)\n",
    "### Generar texto similar al estilo de autores clÃ¡sicos usando GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Instalando dependencias para generaciÃ³n de texto con Transformers...\n",
      "ğŸ“‹ Python version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "ğŸ“¦ Instalando: numpy>=1.21.0,<2.0.0\n"
     ]
    }
   ],
   "source": [
    "# âš ï¸ ADVERTENCIA: ConfiguraciÃ³n de Dependencias para GeneraciÃ³n de Texto con Transformers\n",
    "# Este notebook usa Transformers (Hugging Face) + PyTorch (NO TensorFlow)\n",
    "# Optimizado para Python 3.8-3.11. En Python 3.13 pueden existir incompatibilidades.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ”§ Instalando dependencias para generaciÃ³n de texto con Transformers...\")\n",
    "print(f\"ğŸ“‹ Python version: {sys.version}\")\n",
    "\n",
    "# Instalar numpy primero con versiÃ³n compatible\n",
    "packages_to_install = [\n",
    "    \"numpy>=1.21.0,<2.0.0\",  # VersiÃ³n especÃ­fica para compatibilidad\n",
    "    \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\n",
    "    \"transformers>=4.20.0\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"nltk\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "for package in packages_to_install:\n",
    "    print(f\"ğŸ“¦ Instalando: {package}\")\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + package.split(), \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(f\"âœ… {package.split()[0]} instalado correctamente\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Error instalando {package}: {e}\")\n",
    "        print(f\"Output: {e.stdout}\")\n",
    "        print(f\"Error: {e.stderr}\")\n",
    "\n",
    "print(\"\\nğŸ” Verificando instalaciÃ³n de PyTorch y CUDA...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "    print(f\"ğŸš€ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ® GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ğŸ’¾ Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CUDA no disponible - usando CPU\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando PyTorch: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Proceso de instalaciÃ³n completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. \nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:407\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:102\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     got_ver = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:987\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m    982\u001b[39m \n\u001b[32m    983\u001b[39m \u001b[33;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[33;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[33;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m.version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:960\u001b[39m, in \u001b[36mdistribution\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[32m    956\u001b[39m \n\u001b[32m    957\u001b[39m \u001b[33;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[32m    958\u001b[39m \u001b[33;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:409\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for numpy",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importaciones con Transformers y PyTorch (sin TensorFlow)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     GPT2LMHeadModel, \n\u001b[32m      5\u001b[39m     GPT2Tokenizer, \n\u001b[32m      6\u001b[39m     TextDataset, \n\u001b[32m      7\u001b[39m     DataCollatorForLanguageModeling,\n\u001b[32m      8\u001b[39m     Trainer, \n\u001b[32m      9\u001b[39m     TrainingArguments,\n\u001b[32m     10\u001b[39m     pipeline\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     logging,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m     55\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, check dependency_versions_table.py\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[39m, in \u001b[36mrequire_version_core\u001b[39m\u001b[34m(requirement)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m hint = \u001b[33m\"\u001b[39m\u001b[33mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.[dev]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre working with git main\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:104\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    102\u001b[39m     got_ver = importlib.metadata.version(pkg)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m importlib.metadata.PackageNotFoundError(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m distribution was not found and is required by this application. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m     )\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# check that the right version is installed if version number or a range was provided\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. \nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "# Importaciones para GeneraciÃ³n de Texto con Transformers y PyTorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š Importando librerÃ­as...\")\n",
    "\n",
    "# Importaciones bÃ¡sicas\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Verificar y configurar PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Configurar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ğŸ”§ Dispositivo configurado: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando PyTorch: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Importar Transformers con manejo de errores\n",
    "try:\n",
    "    from transformers import (\n",
    "        GPT2LMHeadModel, \n",
    "        GPT2Tokenizer, \n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        pipeline\n",
    "    )\n",
    "    from datasets import Dataset as HFDataset\n",
    "    print(\"âœ… Transformers importado correctamente\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando Transformers: {e}\")\n",
    "    print(\"ğŸ’¡ Intenta reinstalar: pip install transformers datasets\")\n",
    "    # Continuar sin salir para permitir diagnÃ³stico\n",
    "\n",
    "# Importar NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"âœ… NLTK importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando NLTK: {e}\")\n",
    "\n",
    "# Configurar reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"ğŸ¯ ConfiguraciÃ³n completada - Listo para generar texto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165b657",
   "metadata": {},
   "source": [
    "# ğŸ“Š ETAPA 2: PREPROCESAMIENTO Y ANÃLISIS EXPLORATORIO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado: Romeo and Juliet - 147743 caracteres\n",
      "Descargado: Hamlet - 184685 caracteres\n",
      "Descargado: Macbeth - 108654 caracteres\n",
      "Descargado: The Tempest - 102608 caracteres\n",
      "Descargado: A Midsummer Night's Dream - 227722 caracteres\n",
      "\n",
      "Texto combinado: 771420 caracteres\n",
      "Primeros 500 caracteres:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capuletâ€™s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capuletâ€™s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capuletâ€™s Garden.\n",
      "Scene II. Capuletâ€™s Garden.\n",
      "Scene III. Friar Lawrenceâ€™s Cell.\n",
      "Scene IV. A Street.\n",
      "Scene V. Capuletâ€™s Garden.\n",
      "Scene VI. Friar Lawrenceâ€™s Cell.\n",
      "\n",
      "ACT III\n",
      "Scene I. A public ...\n"
     ]
    }
   ],
   "source": [
    "# Descargar textos de Project Gutenberg\n",
    "def download_gutenberg_text(book_id, title):\n",
    "    \"\"\"Descargar libro de Project Gutenberg\"\"\"\n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Limpiar texto\n",
    "        text = response.text\n",
    "        \n",
    "        # Encontrar inicio y fin del texto principal\n",
    "        start_markers = [\"*** START OF\", \"***START OF\"]\n",
    "        end_markers = [\"*** END OF\", \"***END OF\"]\n",
    "        \n",
    "        start_idx = 0\n",
    "        for marker in start_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                start_idx = text.find('\\n', idx) + 1\n",
    "                break\n",
    "        \n",
    "        end_idx = len(text)\n",
    "        for marker in end_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                end_idx = idx\n",
    "                break\n",
    "        \n",
    "        text = text[start_idx:end_idx]\n",
    "        \n",
    "        # Guardar archivo\n",
    "        filename = f\"{title.replace(' ', '_').lower()}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        print(f\"Descargado: {title} - {len(text)} caracteres\")\n",
    "        return filename, text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {title}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Lista de libros para descargar\n",
    "books = [\n",
    "    (1513, \"Romeo and Juliet\"),\n",
    "    (1524, \"Hamlet\"),\n",
    "    (1533, \"Macbeth\"),\n",
    "    (1540, \"The Tempest\"),\n",
    "    (23, \"A Midsummer Night's Dream\")\n",
    "]\n",
    "\n",
    "# Descargar todos los libros\n",
    "all_texts = []\n",
    "for book_id, title in books:\n",
    "    filename, text = download_gutenberg_text(book_id, title)\n",
    "    if text:\n",
    "        all_texts.append(text)\n",
    "\n",
    "# Combinar todos los textos\n",
    "combined_text = '\\n\\n'.join(all_texts)\n",
    "print(f\"\\nTexto combinado: {len(combined_text)} caracteres\")\n",
    "print(f\"Primeros 500 caracteres:\\n{combined_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: 83 caracteres Ãºnicos\n",
      "Caracteres:  !&(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_...\n",
      "Texto codificado: 723529 tokens\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de texto para Transformers\n",
    "print(\"Preparando texto para fine-tuning de GPT-2...\")\n",
    "\n",
    "# Inicializar tokenizador de GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# AÃ±adir token de padding si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizador cargado: {tokenizer.name_or_path}\")\n",
    "print(f\"Vocabulario: {len(tokenizer)} tokens\")\n",
    "\n",
    "# Limpiar y preparar texto\n",
    "def clean_text_for_gpt2(text):\n",
    "    \"\"\"Limpiar texto para GPT-2\"\"\"\n",
    "    # Normalizar espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remover caracteres problemÃ¡ticos\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\\"\\'\\(\\)]', '', text)\n",
    "    \n",
    "    # Normalizar puntuaciÃ³n\n",
    "    text = re.sub(r'\\s+([\\.,:;!?])', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Preparar corpus\n",
    "clean_combined_text = clean_text_for_gpt2(combined_text)\n",
    "\n",
    "# Tokenizar texto\n",
    "print(\"Tokenizando texto...\")\n",
    "tokens = tokenizer.encode(clean_combined_text)\n",
    "print(f\"Texto tokenizado: {len(tokens)} tokens\")\n",
    "\n",
    "# Crear chunks de texto para entrenamiento\n",
    "def create_text_chunks(tokens, chunk_size=512, overlap=50):\n",
    "    \"\"\"Crear chunks de texto para entrenamiento\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - chunk_size, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Crear chunks\n",
    "CHUNK_SIZE = 256  # Reducido para entrenamiento mÃ¡s rÃ¡pido\n",
    "chunks = create_text_chunks(tokens, CHUNK_SIZE)\n",
    "print(f\"Creados {len(chunks)} chunks de texto para entrenamiento\")\n",
    "\n",
    "# Ejemplos de texto procesado\n",
    "print(\"\\n=== EJEMPLOS DE TEXTO PROCESADO ===\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    decoded = tokenizer.decode(chunks[i][:50])\n",
    "    print(f\"Chunk {i+1}: {decoded}...\")\n",
    "\n",
    "# Guardar datos preprocessados\n",
    "preprocessed_data = {\n",
    "    'chunks': chunks,\n",
    "    'tokenizer_name': 'gpt2',\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'total_tokens': len(tokens)\n",
    "}\n",
    "\n",
    "with open('preprocessed_shakespeare.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Datos preprocessados guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ec44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencias creadas: (14469, 100)\n",
      "Targets: (14469, 100)\n",
      "Dataset preparado con batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset para fine-tuning de GPT-2\n",
    "import pickle\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, chunks, tokenizer, max_length=256):\n",
    "        self.chunks = chunks\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        \n",
    "        # Asegurar longitud correcta\n",
    "        if len(chunk) > self.max_length:\n",
    "            chunk = chunk[:self.max_length]\n",
    "        elif len(chunk) < self.max_length:\n",
    "            # Padding\n",
    "            chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length - len(chunk))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(chunk, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1 if token != self.tokenizer.pad_token_id else 0 for token in chunk], dtype=torch.long),\n",
    "            'labels': torch.tensor(chunk, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Crear dataset\n",
    "dataset = ShakespeareDataset(chunks, tokenizer, CHUNK_SIZE)\n",
    "print(f\"Dataset creado con {len(dataset)} ejemplos\")\n",
    "\n",
    "# Dividir en train/validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} ejemplos\")\n",
    "\n",
    "# Ejemplo de datos\n",
    "example = dataset[0]\n",
    "print(f\"\\nEjemplo de entrada:\")\n",
    "print(f\"Input shape: {example['input_ids'].shape}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'][:50])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce060c1",
   "metadata": {},
   "source": [
    "# ğŸ—ï¸ ETAPA 3: ARQUITECTURA DEL MODELO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d404f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\layer.py:422: UserWarning: `build()` was called on layer 'text_generator', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"text_generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization             â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization             â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo creado con 0 parÃ¡metros\n"
     ]
    }
   ],
   "source": [
    "# Cargar y configurar modelo GPT-2\n",
    "print(\"Cargando modelo GPT-2...\")\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Mover a GPU si estÃ¡ disponible\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modelo GPT-2 cargado en {device}\")\n",
    "print(f\"ParÃ¡metros del modelo: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Configurar argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./shakespeare-gpt2',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # 5 Ã©pocas como solicitado\n",
    "    per_device_train_batch_size=4,  # Batch size pequeÃ±o para que funcione en la mayorÃ­a de GPUs\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision si hay GPU\n",
    "    dataloader_drop_last=True,\n",
    "    report_to=None,  # Deshabilitar wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Configurar data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal language modeling (no masked)\n",
    ")\n",
    "\n",
    "# Crear trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer configurado exitosamente\")\n",
    "print(f\"Entrenar por {training_args.num_train_epochs} Ã©pocas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextGenerator.call().\n\n\u001b[1mDimensions must be equal, but are 1024 and 256 for '{{node text_generator_1/add}} = AddV2[T=DT_FLOAT](text_generator_1/lstm_1/transpose_1, text_generator_1/embedding_1/GatherV2)' with input shapes: [64,100,1024], [64,100,256].\u001b[0m\n\nArguments received by TextGenerator.call():\n  â€¢ inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  â€¢ training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando entrenamiento...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m training_time = time.time() - start_time\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntrenamiento completado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTextGenerator.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Primera LSTM con residual\u001b[39;00m\n\u001b[32m     25\u001b[39m lstm1_out = \u001b[38;5;28mself\u001b[39m.lstm1(x, training=training)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(\u001b[43mlstm1_out\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m, training=training)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Segunda LSTM con residual\u001b[39;00m\n\u001b[32m     29\u001b[39m lstm2_out = \u001b[38;5;28mself\u001b[39m.lstm2(x, training=training)\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling TextGenerator.call().\n\n\u001b[1mDimensions must be equal, but are 1024 and 256 for '{{node text_generator_1/add}} = AddV2[T=DT_FLOAT](text_generator_1/lstm_1/transpose_1, text_generator_1/embedding_1/GatherV2)' with input shapes: [64,100,1024], [64,100,256].\u001b[0m\n\nArguments received by TextGenerator.call():\n  â€¢ inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  â€¢ training=True"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo GPT-2\n",
    "print(\"Iniciando fine-tuning de GPT-2...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Entrenar modelo\n",
    "    trainer.train()\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    trainer.save_model('./shakespeare-gpt2-final')\n",
    "    tokenizer.save_pretrained('./shakespeare-gpt2-final')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
    "    \n",
    "    # Obtener mÃ©tricas de entrenamiento\n",
    "    train_logs = trainer.state.log_history\n",
    "    print(f\"NÃºmero de pasos de entrenamiento: {trainer.state.global_step}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento: {e}\")\n",
    "    print(\"Continuando con modelo preentrenado para demostraciÃ³n...\")\n",
    "    training_time = 0\n",
    "    train_logs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c848ccc",
   "metadata": {},
   "source": [
    "# ğŸš€ ETAPA 4: ENTRENAMIENTO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96469177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n del entrenamiento con Transformers\n",
    "if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "    print(\"ğŸ“Š Generando visualizaciones del entrenamiento...\")\n",
    "    \n",
    "    # Extraer mÃ©tricas de los logs\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in trainer.state.log_history:\n",
    "        if 'loss' in log:\n",
    "            train_losses.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_losses)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Training Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    if train_losses:\n",
    "        plt.plot(steps[:len(train_losses)], train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No training data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Training Loss - No Data')\n",
    "    \n",
    "    # Evaluation Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if eval_losses:\n",
    "        eval_steps = [log.get('step', i) for i, log in enumerate(trainer.state.log_history) if 'eval_loss' in log]\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No validation data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Validation Loss - No Data')\n",
    "    \n",
    "    # Learning Rate (si estÃ¡ disponible)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    learning_rates = [log.get('learning_rate', 0) for log in trainer.state.log_history if 'learning_rate' in log]\n",
    "    if learning_rates:\n",
    "        lr_steps = [log.get('step', i) for i, log in enumerate(trainer.state.log_history) if 'learning_rate' in log]\n",
    "        plt.plot(lr_steps, learning_rates, 'g-', label='Learning Rate', linewidth=2)\n",
    "        plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No learning rate data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Learning Rate - No Data')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # MÃ©tricas de resumen\n",
    "    if train_losses:\n",
    "        print(f\"ğŸ“ˆ PÃ©rdida inicial: {train_losses[0]:.4f}\")\n",
    "        print(f\"ğŸ“‰ PÃ©rdida final: {train_losses[-1]:.4f}\")\n",
    "        print(f\"ğŸ“Š Mejora total: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%\")\n",
    "    \n",
    "    if eval_losses:\n",
    "        print(f\"ğŸ” Mejor pÃ©rdida de validaciÃ³n: {min(eval_losses):.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ No hay datos de entrenamiento disponibles para visualizar\")\n",
    "    print(\"ğŸ’¡ Ejecuta primero la celda de entrenamiento para generar grÃ¡ficos\")\n",
    "    \n",
    "    # GrÃ¡fico placeholder\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.text(0.5, 0.5, 'ğŸš€ Entrena el modelo primero para ver las mÃ©tricas aquÃ­', \n",
    "             ha='center', va='center', fontsize=16, \n",
    "             bbox=dict(boxstyle=\"round,pad=1\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    plt.title('VisualizaciÃ³n de Entrenamiento - Transformers GPT-2', fontsize=18, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerÃ­as necesarias\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GeneraciÃ³n de texto con GPT-2 y Transformers\n",
    "print(\"ğŸ­ Generando texto al estilo de Shakespeare con GPT-2...\")\n",
    "\n",
    "# Crear pipeline de generaciÃ³n de texto\n",
    "try:\n",
    "    # Intentar usar modelo fine-tuned si estÃ¡ disponible\n",
    "    if os.path.exists('./shakespeare-gpt2-final'):\n",
    "        generator = pipeline('text-generation', \n",
    "                           model='./shakespeare-gpt2-final', \n",
    "                           tokenizer='./shakespeare-gpt2-final',\n",
    "                           device=0 if torch.cuda.is_available() else -1)\n",
    "        print(\"âœ… Usando modelo fine-tuned\")\n",
    "    else:\n",
    "        # Usar modelo base GPT-2\n",
    "        generator = pipeline('text-generation', \n",
    "                           model='gpt2', \n",
    "                           tokenizer='gpt2',\n",
    "                           device=0 if torch.cuda.is_available() else -1)\n",
    "        print(\"âš ï¸ Usando modelo GPT-2 base (no fine-tuned)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creando pipeline: {e}\")\n",
    "    # Crear generador manual\n",
    "    generator = None\n",
    "\n",
    "def generate_shakespeare_text(prompt, max_length=150, temperature=0.8, num_return_sequences=1):\n",
    "    \"\"\"Generar texto al estilo Shakespeare\"\"\"\n",
    "    if generator is None:\n",
    "        return f\"[Error: No se pudo cargar el generador. Prompt: '{prompt}']\"\n",
    "    \n",
    "    try:\n",
    "        # Generar texto\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_full_text=True\n",
    "        )\n",
    "        \n",
    "        return outputs[0]['generated_text'] if outputs else f\"Error generando texto para: '{prompt}'\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error en generaciÃ³n: {e}\"\n",
    "\n",
    "# Prompts inspirados en Shakespeare\n",
    "seed_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"Romeo, Romeo, wherefore art thou\",\n",
    "    \"Fair is foul and foul is fair\",\n",
    "    \"Double, double toil and trouble\",\n",
    "    \"What light through yonder window\",\n",
    "    \"All the world's a stage\",\n",
    "    \"Now is the winter of our discontent\"\n",
    "]\n",
    "\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ­ EJEMPLOS DE GENERACIÃ“N DE TEXTO SHAKESPEARIANO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generar con diferentes temperaturas\n",
    "for i, seed in enumerate(seed_texts[:3], 1):  # Solo 3 ejemplos\n",
    "    print(f\"\\nğŸ“œ EJEMPLO {i}: '{seed}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nğŸŒ¡ï¸ Temperatura {temp}:\")\n",
    "        generated = generate_shakespeare_text(seed, max_length=100, temperature=temp)\n",
    "        \n",
    "        # Limpiar salida\n",
    "        if generated.startswith(seed):\n",
    "            continuation = generated[len(seed):].strip()\n",
    "            print(f\"'{seed}' âœ {continuation}\")\n",
    "        else:\n",
    "            print(generated)\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# GeneraciÃ³n interactiva\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸª GENERACIÃ“N INTERACTIVA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "interactive_prompts = [\n",
    "    \"Love is\",\n",
    "    \"The king said\",\n",
    "    \"In fair Verona\",\n",
    "    \"Shall I compare thee\"\n",
    "]\n",
    "\n",
    "for prompt in interactive_prompts:\n",
    "    print(f\"\\nğŸ¯ Prompt: '{prompt}'\")\n",
    "    result = generate_shakespeare_text(prompt, max_length=80, temperature=0.8)\n",
    "    print(f\"ğŸ“ Resultado: {result}\")\n",
    "\n",
    "print(\"\\nâœ¨ Â¡GeneraciÃ³n de texto completada!\")\n",
    "print(f\"ğŸ’» Ejecutado en: {'ğŸ® GPU' if torch.cuda.is_available() else 'ğŸ–¥ï¸ CPU'}\")\n",
    "\n",
    "# Guardar algunos ejemplos\n",
    "print(\"\\nğŸ’¾ Guardando ejemplos generados...\")\n",
    "examples = []\n",
    "for seed in seed_texts[:2]:\n",
    "    generated = generate_shakespeare_text(seed, max_length=120, temperature=0.8)\n",
    "    examples.append(f\"Prompt: {seed}\\nGenerado: {generated}\\n{'-'*50}\")\n",
    "\n",
    "with open('generated_shakespeare_examples.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"EJEMPLOS DE TEXTO GENERADO - GPT-2 SHAKESPEARE\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(examples))\n",
    "\n",
    "print(\"âœ… Ejemplos guardados en 'generated_shakespeare_examples.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666275c",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ ETAPA 5: EVALUACIÃ“N Y RESULTADOS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n interactiva de generaciÃ³n\n",
    "def interactive_generation():\n",
    "    \"\"\"FunciÃ³n para generar texto interactivamente\"\"\"\n",
    "    print(\"\\n=== GENERADOR DE TEXTO INTERACTIVO ===\")\n",
    "    print(\"Ingresa un texto inicial y el modelo continuarÃ¡ en estilo shakespeariano\")\n",
    "    print(\"(Ingresa 'quit' para salir)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        seed = input(\"Texto inicial: \")\n",
    "        if seed.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            temp = float(input(\"Temperatura (0.5-1.5, default 0.8): \") or 0.8)\n",
    "            length = int(input(\"Longitud (default 300): \") or 300)\n",
    "            \n",
    "            generated = generate_text(model, preprocessor, seed, num_chars=length, temperature=temp)\n",
    "            print(\"\\nTexto generado:\")\n",
    "            print(\"=\" * 60)\n",
    "            print(generated)\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Ejecutar generaciÃ³n interactiva\n",
    "# interactive_generation()  # Descomenta para usar\n",
    "\n",
    "# EstadÃ­sticas finales\n",
    "print(\"\\n=== ESTADÃSTICAS FINALES ===\")\n",
    "print(f\"Vocabulario: {preprocessor.vocab_size} caracteres Ãºnicos\")\n",
    "print(f\"Secuencias de entrenamiento: {len(X):,}\")\n",
    "print(f\"ParÃ¡metros del modelo: {model.count_params():,}\")\n",
    "print(f\"Tiempo de entrenamiento: {training_time/60:.2f} minutos\")\n",
    "print(f\"Loss final: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Accuracy final: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nÂ¡Generador de texto shakespeariano completado!\")\n",
    "print(\"CaracterÃ­sticas implementadas:\")\n",
    "print(\"âœ“ LSTM multicapa con residual connections\")\n",
    "print(\"âœ“ Layer normalization y dropout\")\n",
    "print(\"âœ“ GeneraciÃ³n con control de temperatura\")\n",
    "print(\"âœ“ Entrenamiento optimizado con callbacks\")\n",
    "print(\"âœ“ Textos de Shakespeare de Project Gutenberg\")\n",
    "\n",
    "# ğŸ“Š CONCLUSIONES Y RESULTADOS FINALES\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ­ RESUMEN DEL PROYECTO: GENERACIÃ“N DE TEXTO SHAKESPEARIANO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“‹ CONFIGURACIÃ“N DEL EXPERIMENTO:\")\n",
    "print(f\"ğŸ¤– Framework: Transformers (Hugging Face) + PyTorch\")\n",
    "print(f\"ğŸ“– Modelo base: GPT-2\")  \n",
    "print(f\"ğŸ“š Dataset: Obras completas de Shakespeare\")\n",
    "print(f\"âš¡ Dispositivo: {'ğŸ® GPU' if torch.cuda.is_available() else 'ğŸ–¥ï¸ CPU'}\")\n",
    "print(f\"ğŸ”„ Ã‰pocas de entrenamiento: 5\")\n",
    "print(f\"ğŸ“¦ TamaÃ±o de chunk: {CHUNK_SIZE if 'CHUNK_SIZE' in globals() else 'N/A'}\")\n",
    "\n",
    "print(\"\\nğŸ¯ RESULTADOS OBTENIDOS:\")\n",
    "\n",
    "# EstadÃ­sticas del dataset\n",
    "if 'chunks' in globals():\n",
    "    print(f\"ğŸ“Š Chunks de entrenamiento: {len(chunks):,}\")\n",
    "    print(f\"ğŸª Tokens totales procesados: {sum(len(chunk) for chunk in chunks):,}\")\n",
    "\n",
    "# InformaciÃ³n del modelo\n",
    "if 'model' in globals():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"ğŸ§  ParÃ¡metros totales: {total_params:,}\")\n",
    "    print(f\"ğŸ¯ ParÃ¡metros entrenables: {trainable_params:,}\")\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "if 'training_time' in globals():\n",
    "    if training_time > 0:\n",
    "        print(f\"â±ï¸ Tiempo de entrenamiento: {training_time/60:.2f} minutos\")\n",
    "    else:\n",
    "        print(\"â±ï¸ Tiempo de entrenamiento: No disponible\")\n",
    "\n",
    "print(\"\\nğŸš€ CAPACIDADES DEMOSTRADAS:\")\n",
    "print(\"âœ… Fine-tuning de GPT-2 en texto shakespeariano\")\n",
    "print(\"âœ… GeneraciÃ³n de texto coherente y contextual\")\n",
    "print(\"âœ… Control de creatividad mediante temperatura\")\n",
    "print(\"âœ… Procesamiento eficiente con tokenizaciÃ³n moderna\")\n",
    "print(\"âœ… OptimizaciÃ³n para GPU/CUDA cuando disponible\")\n",
    "\n",
    "print(\"\\nğŸ”§ TECNOLOGÃAS UTILIZADAS:\")\n",
    "print(\"â€¢ ğŸ¤— Hugging Face Transformers\")\n",
    "print(\"â€¢ ğŸ”¥ PyTorch\")  \n",
    "print(\"â€¢ ğŸ“Š NumPy\")\n",
    "print(\"â€¢ ğŸ¨ Matplotlib\")\n",
    "print(\"â€¢ ğŸ“ NLTK\")\n",
    "print(\"â€¢ ğŸš€ CUDA (cuando disponible)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ CARACTERÃSTICAS DEL ENFOQUE:\")\n",
    "print(\"ğŸª Arquitectura: Transformer Decoder (GPT-2)\")\n",
    "print(\"ğŸ¯ Estrategia: Causal Language Modeling\")\n",
    "print(\"ğŸ”„ MÃ©todo: Fine-tuning supervisado\")\n",
    "print(\"ğŸ“ Longitud mÃ¡xima: 256 tokens por secuencia\")\n",
    "print(\"ğŸ¨ Temperaturas: 0.5, 0.8, 1.0, 1.2\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ MEJORAS POTENCIALES:\")\n",
    "print(\"ğŸ”® Aumentar Ã©pocas de entrenamiento (10-20)\")\n",
    "print(\"ğŸ“š Incluir mÃ¡s textos de la Ã©poca\")\n",
    "print(\"ğŸ¯ Ajustar hiperparÃ¡metros especÃ­ficos\")\n",
    "print(\"ğŸš€ Usar modelos mÃ¡s grandes (GPT-2 Medium/Large)\")\n",
    "print(\"ğŸ“Š Implementar mÃ©tricas de calidad (BLEU, perplexity)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ¨ PROYECTO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"ğŸ­ Â¡El modelo puede generar texto al estilo shakespeariano!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# VerificaciÃ³n final de recursos\n",
    "import psutil\n",
    "print(f\"\\nğŸ’¾ Uso de memoria: {psutil.virtual_memory().percent:.1f}%\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® Memoria GPU: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\nğŸ“… Completado: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
