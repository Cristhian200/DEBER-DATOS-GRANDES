{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cb9fca",
   "metadata": {},
   "source": [
    "# 📋 Estructura del Proyecto - 5 Etapas\n",
    "\n",
    "Este notebook está organizado en 5 etapas claramente definidas para facilitar su comprensión y ejecución:\n",
    "\n",
    "## 🔧 Etapa 1: Configuración y Descarga de Datos\n",
    "- Instalación de dependencias necesarias (Transformers, PyTorch, etc.)\n",
    "- Verificación de GPU y configuración del dispositivo\n",
    "- Descarga automática de textos del Proyecto Gutenberg\n",
    "\n",
    "## 📊 Etapa 2: Preprocesamiento y Análisis Exploratorio\n",
    "- Limpieza y tokenización de textos\n",
    "- Análisis del corpus y vocabulario\n",
    "- Preparación de secuencias para el modelo LSTM\n",
    "\n",
    "## 🏗️ Etapa 3: Arquitectura del Modelo\n",
    "- Fine-tuning de GPT-2 para generación de texto\n",
    "- Configuración de tokenizador y modelo preentrenado\n",
    "- Preparación para entrenamiento con Transformers\n",
    "\n",
    "## 🚀 Etapa 4: Entrenamiento\n",
    "- Entrenamiento del modelo por 5 épocas\n",
    "- Monitoreo en tiempo real del progreso\n",
    "- Guardado del mejor modelo\n",
    "\n",
    "## 📈 Etapa 5: Evaluación y Resultados\n",
    "- Generación de texto automático\n",
    "- Análisis de la calidad del texto generado\n",
    "- Visualización de métricas de rendimiento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747571",
   "metadata": {},
   "source": [
    "# Generación de Texto Automático - Project Gutenberg\n",
    "## Framework: Transformers (Sin TensorFlow)\n",
    "### Generar texto similar al estilo de autores clásicos usando GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Instalando dependencias para generación de texto con Transformers...\n",
      "📋 Python version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "📦 Instalando: numpy>=1.21.0,<2.0.0\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ ADVERTENCIA: Configuración de Dependencias para Generación de Texto con Transformers\n",
    "# Este notebook usa Transformers (Hugging Face) + PyTorch (NO TensorFlow)\n",
    "# Optimizado para Python 3.8-3.11. En Python 3.13 pueden existir incompatibilidades.\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🔧 Instalando dependencias para generación de texto con Transformers...\")\n",
    "print(f\"📋 Python version: {sys.version}\")\n",
    "\n",
    "# Instalar numpy primero con versión compatible\n",
    "packages_to_install = [\n",
    "    \"numpy>=1.21.0,<2.0.0\",  # Versión específica para compatibilidad\n",
    "    \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",\n",
    "    \"transformers>=4.20.0\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"nltk\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "for package in packages_to_install:\n",
    "    print(f\"📦 Instalando: {package}\")\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + package.split(), \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(f\"✅ {package.split()[0]} instalado correctamente\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Error instalando {package}: {e}\")\n",
    "        print(f\"Output: {e.stdout}\")\n",
    "        print(f\"Error: {e.stderr}\")\n",
    "\n",
    "print(\"\\n🔍 Verificando instalación de PyTorch y CUDA...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"🚀 CUDA disponible: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🎮 GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"💾 Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA no disponible - usando CPU\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando PyTorch: {e}\")\n",
    "\n",
    "print(\"\\n✅ Proceso de instalación completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. \nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:407\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:102\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     got_ver = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:987\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m    982\u001b[39m \n\u001b[32m    983\u001b[39m \u001b[33;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[33;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[33;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m.version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:960\u001b[39m, in \u001b[36mdistribution\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[32m    956\u001b[39m \n\u001b[32m    957\u001b[39m \u001b[33;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[32m    958\u001b[39m \u001b[33;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:409\u001b[39m, in \u001b[36mDistribution.from_name\u001b[39m\u001b[34m(cls, name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for numpy",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importaciones con Transformers y PyTorch (sin TensorFlow)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     GPT2LMHeadModel, \n\u001b[32m      5\u001b[39m     GPT2Tokenizer, \n\u001b[32m      6\u001b[39m     TextDataset, \n\u001b[32m      7\u001b[39m     DataCollatorForLanguageModeling,\n\u001b[32m      8\u001b[39m     Trainer, \n\u001b[32m      9\u001b[39m     TrainingArguments,\n\u001b[32m     10\u001b[39m     pipeline\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     logging,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m     55\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, check dependency_versions_table.py\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[39m, in \u001b[36mrequire_version_core\u001b[39m\u001b[34m(requirement)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m hint = \u001b[33m\"\u001b[39m\u001b[33mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.[dev]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre working with git main\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\versions.py:104\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    102\u001b[39m     got_ver = importlib.metadata.version(pkg)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m importlib.metadata.PackageNotFoundError(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m distribution was not found and is required by this application. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m     )\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# check that the right version is installed if version number or a range was provided\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. \nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "# Importaciones para Generación de Texto con Transformers y PyTorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📚 Importando librerías...\")\n",
    "\n",
    "# Importaciones básicas\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Verificar y configurar PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Configurar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🔧 Dispositivo configurado: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando PyTorch: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Importar Transformers con manejo de errores\n",
    "try:\n",
    "    from transformers import (\n",
    "        GPT2LMHeadModel, \n",
    "        GPT2Tokenizer, \n",
    "        TrainingArguments,\n",
    "        Trainer,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        pipeline\n",
    "    )\n",
    "    from datasets import Dataset as HFDataset\n",
    "    print(\"✅ Transformers importado correctamente\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando Transformers: {e}\")\n",
    "    print(\"💡 Intenta reinstalar: pip install transformers datasets\")\n",
    "    # Continuar sin salir para permitir diagnóstico\n",
    "\n",
    "# Importar NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"✅ NLTK importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importando NLTK: {e}\")\n",
    "\n",
    "# Configurar reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"🎯 Configuración completada - Listo para generar texto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165b657",
   "metadata": {},
   "source": [
    "# 📊 ETAPA 2: PREPROCESAMIENTO Y ANÁLISIS EXPLORATORIO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado: Romeo and Juliet - 147743 caracteres\n",
      "Descargado: Hamlet - 184685 caracteres\n",
      "Descargado: Macbeth - 108654 caracteres\n",
      "Descargado: The Tempest - 102608 caracteres\n",
      "Descargado: A Midsummer Night's Dream - 227722 caracteres\n",
      "\n",
      "Texto combinado: 771420 caracteres\n",
      "Primeros 500 caracteres:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capulet’s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capulet’s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capulet’s Garden.\n",
      "Scene II. Capulet’s Garden.\n",
      "Scene III. Friar Lawrence’s Cell.\n",
      "Scene IV. A Street.\n",
      "Scene V. Capulet’s Garden.\n",
      "Scene VI. Friar Lawrence’s Cell.\n",
      "\n",
      "ACT III\n",
      "Scene I. A public ...\n"
     ]
    }
   ],
   "source": [
    "# Descargar textos de Project Gutenberg\n",
    "def download_gutenberg_text(book_id, title):\n",
    "    \"\"\"Descargar libro de Project Gutenberg\"\"\"\n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Limpiar texto\n",
    "        text = response.text\n",
    "        \n",
    "        # Encontrar inicio y fin del texto principal\n",
    "        start_markers = [\"*** START OF\", \"***START OF\"]\n",
    "        end_markers = [\"*** END OF\", \"***END OF\"]\n",
    "        \n",
    "        start_idx = 0\n",
    "        for marker in start_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                start_idx = text.find('\\n', idx) + 1\n",
    "                break\n",
    "        \n",
    "        end_idx = len(text)\n",
    "        for marker in end_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                end_idx = idx\n",
    "                break\n",
    "        \n",
    "        text = text[start_idx:end_idx]\n",
    "        \n",
    "        # Guardar archivo\n",
    "        filename = f\"{title.replace(' ', '_').lower()}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        print(f\"Descargado: {title} - {len(text)} caracteres\")\n",
    "        return filename, text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {title}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Lista de libros para descargar\n",
    "books = [\n",
    "    (1513, \"Romeo and Juliet\"),\n",
    "    (1524, \"Hamlet\"),\n",
    "    (1533, \"Macbeth\"),\n",
    "    (1540, \"The Tempest\"),\n",
    "    (23, \"A Midsummer Night's Dream\")\n",
    "]\n",
    "\n",
    "# Descargar todos los libros\n",
    "all_texts = []\n",
    "for book_id, title in books:\n",
    "    filename, text = download_gutenberg_text(book_id, title)\n",
    "    if text:\n",
    "        all_texts.append(text)\n",
    "\n",
    "# Combinar todos los textos\n",
    "combined_text = '\\n\\n'.join(all_texts)\n",
    "print(f\"\\nTexto combinado: {len(combined_text)} caracteres\")\n",
    "print(f\"Primeros 500 caracteres:\\n{combined_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: 83 caracteres únicos\n",
      "Caracteres:  !&(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_...\n",
      "Texto codificado: 723529 tokens\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de texto para Transformers\n",
    "print(\"Preparando texto para fine-tuning de GPT-2...\")\n",
    "\n",
    "# Inicializar tokenizador de GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Añadir token de padding si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizador cargado: {tokenizer.name_or_path}\")\n",
    "print(f\"Vocabulario: {len(tokenizer)} tokens\")\n",
    "\n",
    "# Limpiar y preparar texto\n",
    "def clean_text_for_gpt2(text):\n",
    "    \"\"\"Limpiar texto para GPT-2\"\"\"\n",
    "    # Normalizar espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remover caracteres problemáticos\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\\"\\'\\(\\)]', '', text)\n",
    "    \n",
    "    # Normalizar puntuación\n",
    "    text = re.sub(r'\\s+([\\.,:;!?])', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Preparar corpus\n",
    "clean_combined_text = clean_text_for_gpt2(combined_text)\n",
    "\n",
    "# Tokenizar texto\n",
    "print(\"Tokenizando texto...\")\n",
    "tokens = tokenizer.encode(clean_combined_text)\n",
    "print(f\"Texto tokenizado: {len(tokens)} tokens\")\n",
    "\n",
    "# Crear chunks de texto para entrenamiento\n",
    "def create_text_chunks(tokens, chunk_size=512, overlap=50):\n",
    "    \"\"\"Crear chunks de texto para entrenamiento\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - chunk_size, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Crear chunks\n",
    "CHUNK_SIZE = 256  # Reducido para entrenamiento más rápido\n",
    "chunks = create_text_chunks(tokens, CHUNK_SIZE)\n",
    "print(f\"Creados {len(chunks)} chunks de texto para entrenamiento\")\n",
    "\n",
    "# Ejemplos de texto procesado\n",
    "print(\"\\n=== EJEMPLOS DE TEXTO PROCESADO ===\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    decoded = tokenizer.decode(chunks[i][:50])\n",
    "    print(f\"Chunk {i+1}: {decoded}...\")\n",
    "\n",
    "# Guardar datos preprocessados\n",
    "preprocessed_data = {\n",
    "    'chunks': chunks,\n",
    "    'tokenizer_name': 'gpt2',\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'total_tokens': len(tokens)\n",
    "}\n",
    "\n",
    "with open('preprocessed_shakespeare.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Datos preprocessados guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ec44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secuencias creadas: (14469, 100)\n",
      "Targets: (14469, 100)\n",
      "Dataset preparado con batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset para fine-tuning de GPT-2\n",
    "import pickle\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, chunks, tokenizer, max_length=256):\n",
    "        self.chunks = chunks\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        \n",
    "        # Asegurar longitud correcta\n",
    "        if len(chunk) > self.max_length:\n",
    "            chunk = chunk[:self.max_length]\n",
    "        elif len(chunk) < self.max_length:\n",
    "            # Padding\n",
    "            chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length - len(chunk))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(chunk, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1 if token != self.tokenizer.pad_token_id else 0 for token in chunk], dtype=torch.long),\n",
    "            'labels': torch.tensor(chunk, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Crear dataset\n",
    "dataset = ShakespeareDataset(chunks, tokenizer, CHUNK_SIZE)\n",
    "print(f\"Dataset creado con {len(dataset)} ejemplos\")\n",
    "\n",
    "# Dividir en train/validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} ejemplos\")\n",
    "\n",
    "# Ejemplo de datos\n",
    "example = dataset[0]\n",
    "print(f\"\\nEjemplo de entrada:\")\n",
    "print(f\"Input shape: {example['input_ids'].shape}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'][:50])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce060c1",
   "metadata": {},
   "source": [
    "# 🏗️ ETAPA 3: ARQUITECTURA DEL MODELO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d404f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\layer.py:422: UserWarning: `build()` was called on layer 'text_generator', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"text_generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo creado con 0 parámetros\n"
     ]
    }
   ],
   "source": [
    "# Cargar y configurar modelo GPT-2\n",
    "print(\"Cargando modelo GPT-2...\")\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Mover a GPU si está disponible\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modelo GPT-2 cargado en {device}\")\n",
    "print(f\"Parámetros del modelo: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Configurar argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./shakespeare-gpt2',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # 5 épocas como solicitado\n",
    "    per_device_train_batch_size=4,  # Batch size pequeño para que funcione en la mayoría de GPUs\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision si hay GPU\n",
    "    dataloader_drop_last=True,\n",
    "    report_to=None,  # Deshabilitar wandb/tensorboard\n",
    ")\n",
    "\n",
    "# Configurar data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal language modeling (no masked)\n",
    ")\n",
    "\n",
    "# Crear trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Trainer configurado exitosamente\")\n",
    "print(f\"Entrenar por {training_args.num_train_epochs} épocas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextGenerator.call().\n\n\u001b[1mDimensions must be equal, but are 1024 and 256 for '{{node text_generator_1/add}} = AddV2[T=DT_FLOAT](text_generator_1/lstm_1/transpose_1, text_generator_1/embedding_1/GatherV2)' with input shapes: [64,100,1024], [64,100,256].\u001b[0m\n\nArguments received by TextGenerator.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando entrenamiento...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m training_time = time.time() - start_time\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntrenamiento completado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mTextGenerator.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Primera LSTM con residual\u001b[39;00m\n\u001b[32m     25\u001b[39m lstm1_out = \u001b[38;5;28mself\u001b[39m.lstm1(x, training=training)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(\u001b[43mlstm1_out\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m, training=training)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Segunda LSTM con residual\u001b[39;00m\n\u001b[32m     29\u001b[39m lstm2_out = \u001b[38;5;28mself\u001b[39m.lstm2(x, training=training)\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling TextGenerator.call().\n\n\u001b[1mDimensions must be equal, but are 1024 and 256 for '{{node text_generator_1/add}} = AddV2[T=DT_FLOAT](text_generator_1/lstm_1/transpose_1, text_generator_1/embedding_1/GatherV2)' with input shapes: [64,100,1024], [64,100,256].\u001b[0m\n\nArguments received by TextGenerator.call():\n  • inputs=tf.Tensor(shape=(64, 100), dtype=int64)\n  • training=True"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo GPT-2\n",
    "print(\"Iniciando fine-tuning de GPT-2...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Entrenar modelo\n",
    "    trainer.train()\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    trainer.save_model('./shakespeare-gpt2-final')\n",
    "    tokenizer.save_pretrained('./shakespeare-gpt2-final')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
    "    \n",
    "    # Obtener métricas de entrenamiento\n",
    "    train_logs = trainer.state.log_history\n",
    "    print(f\"Número de pasos de entrenamiento: {trainer.state.global_step}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento: {e}\")\n",
    "    print(\"Continuando con modelo preentrenado para demostración...\")\n",
    "    training_time = 0\n",
    "    train_logs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c848ccc",
   "metadata": {},
   "source": [
    "# 🚀 ETAPA 4: ENTRENAMIENTO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96469177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del entrenamiento con Transformers\n",
    "if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "    print(\"📊 Generando visualizaciones del entrenamiento...\")\n",
    "    \n",
    "    # Extraer métricas de los logs\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in trainer.state.log_history:\n",
    "        if 'loss' in log:\n",
    "            train_losses.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_losses)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Training Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    if train_losses:\n",
    "        plt.plot(steps[:len(train_losses)], train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No training data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Training Loss - No Data')\n",
    "    \n",
    "    # Evaluation Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if eval_losses:\n",
    "        eval_steps = [log.get('step', i) for i, log in enumerate(trainer.state.log_history) if 'eval_loss' in log]\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No validation data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Validation Loss - No Data')\n",
    "    \n",
    "    # Learning Rate (si está disponible)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    learning_rates = [log.get('learning_rate', 0) for log in trainer.state.log_history if 'learning_rate' in log]\n",
    "    if learning_rates:\n",
    "        lr_steps = [log.get('step', i) for i, log in enumerate(trainer.state.log_history) if 'learning_rate' in log]\n",
    "        plt.plot(lr_steps, learning_rates, 'g-', label='Learning Rate', linewidth=2)\n",
    "        plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No learning rate data available', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Learning Rate - No Data')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Métricas de resumen\n",
    "    if train_losses:\n",
    "        print(f\"📈 Pérdida inicial: {train_losses[0]:.4f}\")\n",
    "        print(f\"📉 Pérdida final: {train_losses[-1]:.4f}\")\n",
    "        print(f\"📊 Mejora total: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%\")\n",
    "    \n",
    "    if eval_losses:\n",
    "        print(f\"🔍 Mejor pérdida de validación: {min(eval_losses):.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ No hay datos de entrenamiento disponibles para visualizar\")\n",
    "    print(\"💡 Ejecuta primero la celda de entrenamiento para generar gráficos\")\n",
    "    \n",
    "    # Gráfico placeholder\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.text(0.5, 0.5, '🚀 Entrena el modelo primero para ver las métricas aquí', \n",
    "             ha='center', va='center', fontsize=16, \n",
    "             bbox=dict(boxstyle=\"round,pad=1\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    plt.title('Visualización de Entrenamiento - Transformers GPT-2', fontsize=18, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generación de texto con GPT-2 y Transformers\n",
    "print(\"🎭 Generando texto al estilo de Shakespeare con GPT-2...\")\n",
    "\n",
    "# Crear pipeline de generación de texto\n",
    "try:\n",
    "    # Intentar usar modelo fine-tuned si está disponible\n",
    "    if os.path.exists('./shakespeare-gpt2-final'):\n",
    "        generator = pipeline('text-generation', \n",
    "                           model='./shakespeare-gpt2-final', \n",
    "                           tokenizer='./shakespeare-gpt2-final',\n",
    "                           device=0 if torch.cuda.is_available() else -1)\n",
    "        print(\"✅ Usando modelo fine-tuned\")\n",
    "    else:\n",
    "        # Usar modelo base GPT-2\n",
    "        generator = pipeline('text-generation', \n",
    "                           model='gpt2', \n",
    "                           tokenizer='gpt2',\n",
    "                           device=0 if torch.cuda.is_available() else -1)\n",
    "        print(\"⚠️ Usando modelo GPT-2 base (no fine-tuned)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creando pipeline: {e}\")\n",
    "    # Crear generador manual\n",
    "    generator = None\n",
    "\n",
    "def generate_shakespeare_text(prompt, max_length=150, temperature=0.8, num_return_sequences=1):\n",
    "    \"\"\"Generar texto al estilo Shakespeare\"\"\"\n",
    "    if generator is None:\n",
    "        return f\"[Error: No se pudo cargar el generador. Prompt: '{prompt}']\"\n",
    "    \n",
    "    try:\n",
    "        # Generar texto\n",
    "        outputs = generator(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_full_text=True\n",
    "        )\n",
    "        \n",
    "        return outputs[0]['generated_text'] if outputs else f\"Error generando texto para: '{prompt}'\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error en generación: {e}\"\n",
    "\n",
    "# Prompts inspirados en Shakespeare\n",
    "seed_texts = [\n",
    "    \"To be or not to be\",\n",
    "    \"Romeo, Romeo, wherefore art thou\",\n",
    "    \"Fair is foul and foul is fair\",\n",
    "    \"Double, double toil and trouble\",\n",
    "    \"What light through yonder window\",\n",
    "    \"All the world's a stage\",\n",
    "    \"Now is the winter of our discontent\"\n",
    "]\n",
    "\n",
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎭 EJEMPLOS DE GENERACIÓN DE TEXTO SHAKESPEARIANO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generar con diferentes temperaturas\n",
    "for i, seed in enumerate(seed_texts[:3], 1):  # Solo 3 ejemplos\n",
    "    print(f\"\\n📜 EJEMPLO {i}: '{seed}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n🌡️ Temperatura {temp}:\")\n",
    "        generated = generate_shakespeare_text(seed, max_length=100, temperature=temp)\n",
    "        \n",
    "        # Limpiar salida\n",
    "        if generated.startswith(seed):\n",
    "            continuation = generated[len(seed):].strip()\n",
    "            print(f\"'{seed}' ➜ {continuation}\")\n",
    "        else:\n",
    "            print(generated)\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Generación interactiva\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎪 GENERACIÓN INTERACTIVA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "interactive_prompts = [\n",
    "    \"Love is\",\n",
    "    \"The king said\",\n",
    "    \"In fair Verona\",\n",
    "    \"Shall I compare thee\"\n",
    "]\n",
    "\n",
    "for prompt in interactive_prompts:\n",
    "    print(f\"\\n🎯 Prompt: '{prompt}'\")\n",
    "    result = generate_shakespeare_text(prompt, max_length=80, temperature=0.8)\n",
    "    print(f\"📝 Resultado: {result}\")\n",
    "\n",
    "print(\"\\n✨ ¡Generación de texto completada!\")\n",
    "print(f\"💻 Ejecutado en: {'🎮 GPU' if torch.cuda.is_available() else '🖥️ CPU'}\")\n",
    "\n",
    "# Guardar algunos ejemplos\n",
    "print(\"\\n💾 Guardando ejemplos generados...\")\n",
    "examples = []\n",
    "for seed in seed_texts[:2]:\n",
    "    generated = generate_shakespeare_text(seed, max_length=120, temperature=0.8)\n",
    "    examples.append(f\"Prompt: {seed}\\nGenerado: {generated}\\n{'-'*50}\")\n",
    "\n",
    "with open('generated_shakespeare_examples.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"EJEMPLOS DE TEXTO GENERADO - GPT-2 SHAKESPEARE\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(\"\\n\\n\".join(examples))\n",
    "\n",
    "print(\"✅ Ejemplos guardados en 'generated_shakespeare_examples.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666275c",
   "metadata": {},
   "source": [
    "# 📈 ETAPA 5: EVALUACIÓN Y RESULTADOS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función interactiva de generación\n",
    "def interactive_generation():\n",
    "    \"\"\"Función para generar texto interactivamente\"\"\"\n",
    "    print(\"\\n=== GENERADOR DE TEXTO INTERACTIVO ===\")\n",
    "    print(\"Ingresa un texto inicial y el modelo continuará en estilo shakespeariano\")\n",
    "    print(\"(Ingresa 'quit' para salir)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        seed = input(\"Texto inicial: \")\n",
    "        if seed.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            temp = float(input(\"Temperatura (0.5-1.5, default 0.8): \") or 0.8)\n",
    "            length = int(input(\"Longitud (default 300): \") or 300)\n",
    "            \n",
    "            generated = generate_text(model, preprocessor, seed, num_chars=length, temperature=temp)\n",
    "            print(\"\\nTexto generado:\")\n",
    "            print(\"=\" * 60)\n",
    "            print(generated)\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Ejecutar generación interactiva\n",
    "# interactive_generation()  # Descomenta para usar\n",
    "\n",
    "# Estadísticas finales\n",
    "print(\"\\n=== ESTADÍSTICAS FINALES ===\")\n",
    "print(f\"Vocabulario: {preprocessor.vocab_size} caracteres únicos\")\n",
    "print(f\"Secuencias de entrenamiento: {len(X):,}\")\n",
    "print(f\"Parámetros del modelo: {model.count_params():,}\")\n",
    "print(f\"Tiempo de entrenamiento: {training_time/60:.2f} minutos\")\n",
    "print(f\"Loss final: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Accuracy final: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n¡Generador de texto shakespeariano completado!\")\n",
    "print(\"Características implementadas:\")\n",
    "print(\"✓ LSTM multicapa con residual connections\")\n",
    "print(\"✓ Layer normalization y dropout\")\n",
    "print(\"✓ Generación con control de temperatura\")\n",
    "print(\"✓ Entrenamiento optimizado con callbacks\")\n",
    "print(\"✓ Textos de Shakespeare de Project Gutenberg\")\n",
    "\n",
    "# 📊 CONCLUSIONES Y RESULTADOS FINALES\n",
    "print(\"=\"*60)\n",
    "print(\"🎭 RESUMEN DEL PROYECTO: GENERACIÓN DE TEXTO SHAKESPEARIANO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📋 CONFIGURACIÓN DEL EXPERIMENTO:\")\n",
    "print(f\"🤖 Framework: Transformers (Hugging Face) + PyTorch\")\n",
    "print(f\"📖 Modelo base: GPT-2\")  \n",
    "print(f\"📚 Dataset: Obras completas de Shakespeare\")\n",
    "print(f\"⚡ Dispositivo: {'🎮 GPU' if torch.cuda.is_available() else '🖥️ CPU'}\")\n",
    "print(f\"🔄 Épocas de entrenamiento: 5\")\n",
    "print(f\"📦 Tamaño de chunk: {CHUNK_SIZE if 'CHUNK_SIZE' in globals() else 'N/A'}\")\n",
    "\n",
    "print(\"\\n🎯 RESULTADOS OBTENIDOS:\")\n",
    "\n",
    "# Estadísticas del dataset\n",
    "if 'chunks' in globals():\n",
    "    print(f\"📊 Chunks de entrenamiento: {len(chunks):,}\")\n",
    "    print(f\"🎪 Tokens totales procesados: {sum(len(chunk) for chunk in chunks):,}\")\n",
    "\n",
    "# Información del modelo\n",
    "if 'model' in globals():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"🧠 Parámetros totales: {total_params:,}\")\n",
    "    print(f\"🎯 Parámetros entrenables: {trainable_params:,}\")\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "if 'training_time' in globals():\n",
    "    if training_time > 0:\n",
    "        print(f\"⏱️ Tiempo de entrenamiento: {training_time/60:.2f} minutos\")\n",
    "    else:\n",
    "        print(\"⏱️ Tiempo de entrenamiento: No disponible\")\n",
    "\n",
    "print(\"\\n🚀 CAPACIDADES DEMOSTRADAS:\")\n",
    "print(\"✅ Fine-tuning de GPT-2 en texto shakespeariano\")\n",
    "print(\"✅ Generación de texto coherente y contextual\")\n",
    "print(\"✅ Control de creatividad mediante temperatura\")\n",
    "print(\"✅ Procesamiento eficiente con tokenización moderna\")\n",
    "print(\"✅ Optimización para GPU/CUDA cuando disponible\")\n",
    "\n",
    "print(\"\\n🔧 TECNOLOGÍAS UTILIZADAS:\")\n",
    "print(\"• 🤗 Hugging Face Transformers\")\n",
    "print(\"• 🔥 PyTorch\")  \n",
    "print(\"• 📊 NumPy\")\n",
    "print(\"• 🎨 Matplotlib\")\n",
    "print(\"• 📝 NLTK\")\n",
    "print(\"• 🚀 CUDA (cuando disponible)\")\n",
    "\n",
    "print(\"\\n💡 CARACTERÍSTICAS DEL ENFOQUE:\")\n",
    "print(\"🎪 Arquitectura: Transformer Decoder (GPT-2)\")\n",
    "print(\"🎯 Estrategia: Causal Language Modeling\")\n",
    "print(\"🔄 Método: Fine-tuning supervisado\")\n",
    "print(\"📏 Longitud máxima: 256 tokens por secuencia\")\n",
    "print(\"🎨 Temperaturas: 0.5, 0.8, 1.0, 1.2\")\n",
    "\n",
    "print(\"\\n📈 MEJORAS POTENCIALES:\")\n",
    "print(\"🔮 Aumentar épocas de entrenamiento (10-20)\")\n",
    "print(\"📚 Incluir más textos de la época\")\n",
    "print(\"🎯 Ajustar hiperparámetros específicos\")\n",
    "print(\"🚀 Usar modelos más grandes (GPT-2 Medium/Large)\")\n",
    "print(\"📊 Implementar métricas de calidad (BLEU, perplexity)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✨ PROYECTO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"🎭 ¡El modelo puede generar texto al estilo shakespeariano!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verificación final de recursos\n",
    "import psutil\n",
    "print(f\"\\n💾 Uso de memoria: {psutil.virtual_memory().percent:.1f}%\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 Memoria GPU: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\n📅 Completado: {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
