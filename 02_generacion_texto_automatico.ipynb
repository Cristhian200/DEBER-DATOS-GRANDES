{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2d62834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verificando Transformers paso a paso...\n",
      "Paso 1: Importando transformers...\n",
      "âœ… Transformers 4.53.0 importado\n",
      "Paso 2: Importando GPT2Tokenizer...\n",
      "âœ… GPT2Tokenizer importado\n",
      "Paso 3: Creando tokenizer...\n",
      "âŒ Error: Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?\n",
      "ðŸ“ Tipo: ModuleNotFoundError\n",
      "\n",
      "ðŸ” Debug info:\n",
      "- Python: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "- MÃ³dulos cargados: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'winreg', '_io', 'marshal']...\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§ª PRUEBA SIMPLE DE TRANSFORMERS - Python 3.13\n",
    "# VerificaciÃ³n bÃ¡sica antes de continuar\n",
    "\n",
    "print(\"ðŸ” Verificando Transformers paso a paso...\")\n",
    "\n",
    "try:\n",
    "    print(\"Paso 1: Importando transformers...\")\n",
    "    import transformers\n",
    "    print(f\"âœ… Transformers {transformers.__version__} importado\")\n",
    "    \n",
    "    print(\"Paso 2: Importando GPT2Tokenizer...\")\n",
    "    from transformers import GPT2Tokenizer\n",
    "    print(\"âœ… GPT2Tokenizer importado\")\n",
    "    \n",
    "    print(\"Paso 3: Creando tokenizer...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    print(\"âœ… Tokenizer creado exitosamente\")\n",
    "    \n",
    "    print(\"Paso 4: Probando tokenizaciÃ³n...\")\n",
    "    texto = \"Hello, world!\"\n",
    "    tokens = tokenizer.encode(texto)\n",
    "    print(f\"âœ… Texto tokenizado: {tokens}\")\n",
    "    \n",
    "    print(\"Paso 5: Importando modelo...\")\n",
    "    from transformers import GPT2LMHeadModel\n",
    "    print(\"âœ… GPT2LMHeadModel importado\")\n",
    "    \n",
    "    print(\"ðŸŽ‰ Â¡TRANSFORMERS FUNCIONA PERFECTAMENTE!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(f\"ðŸ“ Tipo: {type(e).__name__}\")\n",
    "    \n",
    "    # InformaciÃ³n adicional de debug\n",
    "    import sys\n",
    "    print(f\"\\nðŸ” Debug info:\")\n",
    "    print(f\"- Python: {sys.version}\")\n",
    "    print(f\"- MÃ³dulos cargados: {list(sys.modules.keys())[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb9fca",
   "metadata": {
    "id": "12cb9fca"
   },
   "source": [
    "# ðŸ“‹ Estructura del Proyecto - 5 Etapas\n",
    "\n",
    "Este notebook estÃ¡ organizado en 5 etapas claramente definidas para facilitar su comprensiÃ³n y ejecuciÃ³n:\n",
    "\n",
    "## ðŸ”§ Etapa 1: ConfiguraciÃ³n y Descarga de Datos\n",
    "- InstalaciÃ³n de dependencias necesarias (Transformers, PyTorch, etc.)\n",
    "- VerificaciÃ³n de GPU y configuraciÃ³n del dispositivo\n",
    "- Descarga automÃ¡tica de textos del Proyecto Gutenberg\n",
    "\n",
    "## ðŸ“Š Etapa 2: Preprocesamiento y AnÃ¡lisis Exploratorio\n",
    "- Limpieza y tokenizaciÃ³n de textos\n",
    "- AnÃ¡lisis del corpus y vocabulario\n",
    "- PreparaciÃ³n de secuencias para el modelo LSTM\n",
    "\n",
    "## ðŸ—ï¸ Etapa 3: Arquitectura del Modelo\n",
    "- Fine-tuning de GPT-2 para generaciÃ³n de texto\n",
    "- ConfiguraciÃ³n de tokenizador y modelo preentrenado\n",
    "- PreparaciÃ³n para entrenamiento con Transformers\n",
    "\n",
    "## ðŸš€ Etapa 4: Entrenamiento\n",
    "- Entrenamiento del modelo por 5 Ã©pocas\n",
    "- Monitoreo en tiempo real del progreso\n",
    "- Guardado del mejor modelo\n",
    "\n",
    "## ðŸ“ˆ Etapa 5: EvaluaciÃ³n y Resultados\n",
    "- GeneraciÃ³n de texto automÃ¡tico\n",
    "- AnÃ¡lisis de la calidad del texto generado\n",
    "- VisualizaciÃ³n de mÃ©tricas de rendimiento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747571",
   "metadata": {
    "id": "fc747571"
   },
   "source": [
    "# GeneraciÃ³n de Texto AutomÃ¡tico - Project Gutenberg\n",
    "## Framework: Transformers (Sin TensorFlow)\n",
    "### Generar texto similar al estilo de autores clÃ¡sicos usando GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "807f1343",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "807f1343",
    "outputId": "9f9340dc-ea48-45ae-ba40-784dc450e11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configurando entorno para generaciÃ³n de texto...\n",
      "ðŸ“‹ Python version: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "âœ… Python 3.13 detectado - configurando paquetes compatibles\n",
      "\n",
      "ðŸ“¦ Instalando dependencias...\n",
      "   ðŸ“¦ Instalando: numpy==1.26.4\n",
      "   âœ… numpy instalado correctamente\n",
      "   ðŸ“¦ Instalando: scipy==1.13.1\n",
      "   âœ… numpy instalado correctamente\n",
      "   ðŸ“¦ Instalando: scipy==1.13.1\n",
      "   âŒ Error con scipy==1.13.1:   error: subprocess-exited-with-error\n",
      "  \n",
      "  Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  Ã¢â€â€š exit code: 1\n",
      "  Ã¢â€¢Â°Ã¢â€â‚¬> [47 lines of output]\n",
      "      + meson setup C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.8.2\n",
      "      Source dir: C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\n",
      "      Build dir: C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\n",
      "      Build type: native build\n",
      "      Activating VS 17.14.5\n",
      "      Project name: scipy\n",
      "      Project version: 1.13.1\n",
      "      C compiler for the host machine: cl (msvc 19.44.35209 \"Compilador de optimizaciÃƒÂ¯Ã‚Â¿Ã‚Â½n de C/C++ de Microsoft (R) versiÃƒÂ¯Ã‚Â¿Ã‚Â½n 19.44.35209 para x64\")\n",
      "      C linker for the host machine: link link 14.44.35209.0\n",
      "      C++ compiler for the host machine: cl (msvc 19.44.35209 \"Compilador de optimizaciÃƒÂ¯Ã‚Â¿Ã‚Â½n de C/C++ de Microsoft (R) versiÃƒÂ¯Ã‚Â¿Ã‚Â½n 19.44.35209 para x64\")\n",
      "      C++ linker for the host machine: link link 14.44.35209.0\n",
      "      Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "      Host machine cpu family: x86_64\n",
      "      Host machine cpu: x86_64\n",
      "      Program python found: YES (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n",
      "      Run-time dependency python found: YES 3.13\n",
      "      Program cython found: YES (C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-build-env-ib458ox2\\overlay\\Scripts\\cython.EXE)\n",
      "      Compiler for C supports arguments -Wno-unused-but-set-variable: NO\n",
      "      Compiler for C supports arguments -Wno-unused-function: NO\n",
      "      Compiler for C supports arguments -Wno-conversion: NO\n",
      "      Compiler for C supports arguments -Wno-misleading-indentation: NO\n",
      "      Library m found: NO\n",
      "      \n",
      "      ..\\meson.build:78:0: ERROR: Unknown compiler(s): [['ifort'], ['gfortran'], ['flang-new'], ['flang'], ['pgfortran'], ['g95']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `ifort --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `ifort --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `ifort -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ãƒâ€” Encountered error while generating package metadata.\n",
      "Ã¢â€¢Â°Ã¢â€â‚¬> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "   ðŸ“¦ Instalando: scikit-learn==1.5.2\n",
      "   âŒ Error con scipy==1.13.1:   error: subprocess-exited-with-error\n",
      "  \n",
      "  Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  Ã¢â€â€š exit code: 1\n",
      "  Ã¢â€¢Â°Ã¢â€â‚¬> [47 lines of output]\n",
      "      + meson setup C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.8.2\n",
      "      Source dir: C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\n",
      "      Build dir: C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\n",
      "      Build type: native build\n",
      "      Activating VS 17.14.5\n",
      "      Project name: scipy\n",
      "      Project version: 1.13.1\n",
      "      C compiler for the host machine: cl (msvc 19.44.35209 \"Compilador de optimizaciÃƒÂ¯Ã‚Â¿Ã‚Â½n de C/C++ de Microsoft (R) versiÃƒÂ¯Ã‚Â¿Ã‚Â½n 19.44.35209 para x64\")\n",
      "      C linker for the host machine: link link 14.44.35209.0\n",
      "      C++ compiler for the host machine: cl (msvc 19.44.35209 \"Compilador de optimizaciÃƒÂ¯Ã‚Â¿Ã‚Â½n de C/C++ de Microsoft (R) versiÃƒÂ¯Ã‚Â¿Ã‚Â½n 19.44.35209 para x64\")\n",
      "      C++ linker for the host machine: link link 14.44.35209.0\n",
      "      Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "      Host machine cpu family: x86_64\n",
      "      Host machine cpu: x86_64\n",
      "      Program python found: YES (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\python.exe)\n",
      "      Run-time dependency python found: YES 3.13\n",
      "      Program cython found: YES (C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-build-env-ib458ox2\\overlay\\Scripts\\cython.EXE)\n",
      "      Compiler for C supports arguments -Wno-unused-but-set-variable: NO\n",
      "      Compiler for C supports arguments -Wno-unused-function: NO\n",
      "      Compiler for C supports arguments -Wno-conversion: NO\n",
      "      Compiler for C supports arguments -Wno-misleading-indentation: NO\n",
      "      Library m found: NO\n",
      "      \n",
      "      ..\\meson.build:78:0: ERROR: Unknown compiler(s): [['ifort'], ['gfortran'], ['flang-new'], ['flang'], ['pgfortran'], ['g95']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `ifort --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `ifort --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `ifort -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `gfortran -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang-new -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `flang -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `pgfortran -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 --help` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 --version` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      Running `g95 -V` gave \"[WinError 2] El sistema no puede encontrar el archivo especificado\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Cristhian Ismael\\AppData\\Local\\Temp\\pip-install-lc7uz0bp\\scipy_4631eb5792ee453994d318759064674f\\.mesonpy-tjp3y3l8\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ãƒâ€” Encountered error while generating package metadata.\n",
      "Ã¢â€¢Â°Ã¢â€â‚¬> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "   ðŸ“¦ Instalando: scikit-learn==1.5.2\n",
      "   âœ… scikit-learn instalado correctamente\n",
      "   ðŸ“¦ Instalando: torch==2.5.1\n",
      "   âœ… scikit-learn instalado correctamente\n",
      "   ðŸ“¦ Instalando: torch==2.5.1\n",
      "   âŒ Error con torch==2.5.1: ERROR: Could not find a version that satisfies the requirement torch==2.5.1 (from versions: 2.6.0, 2.7.0, 2.7.1)\n",
      "ERROR: No matching distribution found for torch==2.5.1\n",
      "\n",
      "   ðŸ”„ Intentando instalaciÃ³n alternativa para torch==2.5.1...\n",
      "   âŒ Error con torch==2.5.1: ERROR: Could not find a version that satisfies the requirement torch==2.5.1 (from versions: 2.6.0, 2.7.0, 2.7.1)\n",
      "ERROR: No matching distribution found for torch==2.5.1\n",
      "\n",
      "   ðŸ”„ Intentando instalaciÃ³n alternativa para torch==2.5.1...\n",
      "   âœ… torch instalado con versiÃ³n alternativa\n",
      "   ðŸ“¦ Instalando: transformers==4.45.2\n",
      "   âœ… torch instalado con versiÃ³n alternativa\n",
      "   ðŸ“¦ Instalando: transformers==4.45.2\n",
      "   âœ… transformers instalado correctamente\n",
      "   ðŸ“¦ Instalando: tokenizers==0.20.1\n",
      "   âœ… transformers instalado correctamente\n",
      "   ðŸ“¦ Instalando: tokenizers==0.20.1\n",
      "   âŒ Error con tokenizers==0.20.1:   error: subprocess-exited-with-error\n",
      "  \n",
      "  Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  Ã¢â€â€š exit code: 1\n",
      "  Ã¢â€¢Â°Ã¢â€â‚¬> [29 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\Cristhian Ismael\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Downloading rustup-init from https://static.rust-lang.org/rustup/dist/x86_64-pc-windows-msvc/rustup-init.exe\n",
      "      \n",
      "      Downloading rustup-init:   0%|          | 0.00/13.6M [00:00<?, ?B/s]\n",
      "      Downloading rustup-init:  22%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œ\\x8f       | 2.94M/13.6M [00:00<00:00, 29.3MB/s]\n",
      "      Downloading rustup-init:  92%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œ\\x8f| 12.5M/13.6M [00:00<00:00, 68.2MB/s]\n",
      "      Downloading rustup-init: 100%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ | 13.6M/13.6M [00:00<00:00, 64.9MB/s]\n",
      "      Installing rust to C:\\Users\\Cristhian Ismael\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: latest update on 2025-06-26, rust version 1.88.0 (6b00bc388 2025-06-23)\n",
      "      info: downloading component 'cargo'\n",
      "      info: downloading component 'rust-std'\n",
      "      info: downloading component 'rustc'\n",
      "      info: installing component 'cargo'\n",
      "      info: installing component 'rust-std'\n",
      "      info: installing component 'rustc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ãƒâ€” Encountered error while generating package metadata.\n",
      "Ã¢â€¢Â°Ã¢â€â‚¬> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "   ðŸ“¦ Instalando: matplotlib==3.9.2\n",
      "   âŒ Error con tokenizers==0.20.1:   error: subprocess-exited-with-error\n",
      "  \n",
      "  Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  Ã¢â€â€š exit code: 1\n",
      "  Ã¢â€¢Â°Ã¢â€â‚¬> [29 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\Cristhian Ismael\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Downloading rustup-init from https://static.rust-lang.org/rustup/dist/x86_64-pc-windows-msvc/rustup-init.exe\n",
      "      \n",
      "      Downloading rustup-init:   0%|          | 0.00/13.6M [00:00<?, ?B/s]\n",
      "      Downloading rustup-init:  22%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œ\\x8f       | 2.94M/13.6M [00:00<00:00, 29.3MB/s]\n",
      "      Downloading rustup-init:  92%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œ\\x8f| 12.5M/13.6M [00:00<00:00, 68.2MB/s]\n",
      "      Downloading rustup-init: 100%|ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ ÃƒÂ¢Ã¢â‚¬â€œÃ‹â€ | 13.6M/13.6M [00:00<00:00, 64.9MB/s]\n",
      "      Installing rust to C:\\Users\\Cristhian Ismael\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: latest update on 2025-06-26, rust version 1.88.0 (6b00bc388 2025-06-23)\n",
      "      info: downloading component 'cargo'\n",
      "      info: downloading component 'rust-std'\n",
      "      info: downloading component 'rustc'\n",
      "      info: installing component 'cargo'\n",
      "      info: installing component 'rust-std'\n",
      "      info: installing component 'rustc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ãƒâ€” Encountered error while generating package metadata.\n",
      "Ã¢â€¢Â°Ã¢â€â‚¬> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "   ðŸ“¦ Instalando: matplotlib==3.9.2\n",
      "   âŒ Error con matplotlib==3.9.2: ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\users\\\\cristhian ismael\\\\appdata\\\\local\\\\programs\\\\python\\\\python313\\\\lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "   ðŸ“¦ Instalando: seaborn==0.13.2\n",
      "   âŒ Error con matplotlib==3.9.2: ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\users\\\\cristhian ismael\\\\appdata\\\\local\\\\programs\\\\python\\\\python313\\\\lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "   ðŸ“¦ Instalando: seaborn==0.13.2\n",
      "   âŒ Error con seaborn==0.13.2: WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\Users\\\\Cristhian Ismael\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "   ðŸ“¦ Instalando: tqdm==4.66.5\n",
      "   âŒ Error con seaborn==0.13.2: WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\Users\\\\Cristhian Ismael\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "   ðŸ“¦ Instalando: tqdm==4.66.5\n",
      "   âœ… tqdm instalado correctamente\n",
      "   ðŸ“¦ Instalando: requests==2.32.3\n",
      "   âœ… tqdm instalado correctamente\n",
      "   ðŸ“¦ Instalando: requests==2.32.3\n",
      "   âœ… requests instalado correctamente\n",
      "   ðŸ“¦ Instalando: datasets==3.0.1\n",
      "   âœ… requests instalado correctamente\n",
      "   ðŸ“¦ Instalando: datasets==3.0.1\n",
      "   âœ… datasets instalado correctamente\n",
      "   ðŸ“¦ Instalando: accelerate==1.0.1\n",
      "   âœ… datasets instalado correctamente\n",
      "   ðŸ“¦ Instalando: accelerate==1.0.1\n",
      "   âœ… accelerate instalado correctamente\n",
      "   ðŸ“¦ Instalando: nltk==3.9.1\n",
      "   âœ… accelerate instalado correctamente\n",
      "   ðŸ“¦ Instalando: nltk==3.9.1\n",
      "   âœ… nltk instalado correctamente\n",
      "\n",
      "âš ï¸ Algunos paquetes fallaron: ['scipy==1.13.1', 'tokenizers==0.20.1', 'matplotlib==3.9.2', 'seaborn==0.13.2']\n",
      "ðŸ’¡ El notebook continuarÃ¡ con funcionalidad limitada\n",
      "\n",
      "ðŸ” Verificando instalaciÃ³n...\n",
      "âœ… PyTorch: 2.7.1+cu118\n",
      "ðŸš€ CUDA disponible: True\n",
      "ðŸŽ® GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "\n",
      "âœ… ConfiguraciÃ³n completada!\n",
      "   âœ… nltk instalado correctamente\n",
      "\n",
      "âš ï¸ Algunos paquetes fallaron: ['scipy==1.13.1', 'tokenizers==0.20.1', 'matplotlib==3.9.2', 'seaborn==0.13.2']\n",
      "ðŸ’¡ El notebook continuarÃ¡ con funcionalidad limitada\n",
      "\n",
      "ðŸ” Verificando instalaciÃ³n...\n",
      "âœ… PyTorch: 2.7.1+cu118\n",
      "ðŸš€ CUDA disponible: True\n",
      "ðŸŽ® GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "\n",
      "âœ… ConfiguraciÃ³n completada!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ CONFIGURACIÃ“N COMPATIBLE - GENERACIÃ“N DE TEXTO\n",
    "# VersiÃ³n optimizada para Python 3.8-3.13 con manejo de incompatibilidades\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ”§ Configurando entorno para generaciÃ³n de texto...\")\n",
    "print(f\"ðŸ“‹ Python version: {sys.version}\")\n",
    "\n",
    "# Detectar versiÃ³n de Python\n",
    "python_version = sys.version_info\n",
    "is_python_313 = python_version >= (3, 13)\n",
    "\n",
    "if is_python_313:\n",
    "    print(\"âœ… Python 3.13 detectado - configurando paquetes compatibles\")\n",
    "    packages_to_install = [\n",
    "        \"numpy==1.26.4\",  # VersiÃ³n especÃ­fica compatible con Python 3.13\n",
    "        \"scipy==1.13.1\",  # VersiÃ³n especÃ­fica compatible\n",
    "        \"scikit-learn==1.5.2\",  # VersiÃ³n especÃ­fica compatible\n",
    "        \"torch==2.5.1\",  # VersiÃ³n mÃ¡s reciente compatible\n",
    "        \"transformers==4.45.2\",  # VersiÃ³n especÃ­fica que funciona con las dependencias\n",
    "        \"tokenizers==0.20.1\",  # VersiÃ³n compatible\n",
    "        \"matplotlib==3.9.2\",\n",
    "        \"seaborn==0.13.2\", \n",
    "        \"tqdm==4.66.5\",\n",
    "        \"requests==2.32.3\",\n",
    "        \"datasets==3.0.1\",\n",
    "        \"accelerate==1.0.1\",\n",
    "        \"nltk==3.9.1\"\n",
    "    ]\n",
    "else:\n",
    "    print(\"âœ… Python compatible detectado - usando versiones estÃ¡ndar\")\n",
    "    packages_to_install = [\n",
    "        \"numpy>=1.21.0,<2.0.0\",\n",
    "        \"torch\",\n",
    "        \"transformers>=4.20.0\",\n",
    "        \"datasets\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"scikit-learn\", \n",
    "        \"tqdm\",\n",
    "        \"requests\",\n",
    "        \"nltk\"\n",
    "    ]\n",
    "\n",
    "print(\"\\nðŸ“¦ Instalando dependencias...\")\n",
    "failed_packages = []\n",
    "\n",
    "for package in packages_to_install:\n",
    "    print(f\"   ðŸ“¦ Instalando: {package}\")\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package, \"--no-cache-dir\", \"-q\"],\n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(f\"   âœ… {package.split('==')[0].split('>=')[0]} instalado correctamente\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"   âŒ Error con {package}: {e.stderr if e.stderr else 'Error desconocido'}\")\n",
    "        failed_packages.append(package)\n",
    "        \n",
    "        # Intentar instalaciÃ³n alternativa para paquetes crÃ­ticos\n",
    "        if any(critical in package for critical in ['numpy', 'torch', 'transformers']):\n",
    "            print(f\"   ðŸ”„ Intentando instalaciÃ³n alternativa para {package}...\")\n",
    "            try:\n",
    "                base_package = package.split('==')[0].split('>=')[0]\n",
    "                result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", base_package, \"--upgrade\", \"--no-cache-dir\", \"-q\"],\n",
    "                                      capture_output=True, text=True, check=True)\n",
    "                print(f\"   âœ… {base_package} instalado con versiÃ³n alternativa\")\n",
    "                failed_packages.remove(package)\n",
    "            except:\n",
    "                print(f\"   âŒ InstalaciÃ³n alternativa fallÃ³ para {base_package}\")\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\nâš ï¸ Algunos paquetes fallaron: {failed_packages}\")\n",
    "    print(\"ðŸ’¡ El notebook continuarÃ¡ con funcionalidad limitada\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ Todas las dependencias instaladas correctamente!\")\n",
    "\n",
    "print(\"\\nðŸ” Verificando instalaciÃ³n...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"ðŸš€ CUDA disponible: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"ðŸ’» Usando CPU\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando PyTorch: {e}\")\n",
    "\n",
    "print(\"\\nâœ… ConfiguraciÃ³n completada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7627a286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7627a286",
    "outputId": "ad543740-a21f-461d-b77b-3094172b4b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Importando solo librerÃ­as compatibles...\n",
      "âœ… LibrerÃ­as bÃ¡sicas importadas correctamente\n",
      "   - NumPy: 2.2.6\n",
      "âœ… PyTorch: 2.7.1+cu118\n",
      "ðŸ”§ Dispositivo configurado: cuda\n",
      "ðŸŽ® GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "âœ… Clases personalizadas definidas\n",
      "âœ… Dataset personalizado definido\n",
      "âœ… FunciÃ³n de generaciÃ³n definida\n",
      "\n",
      "ðŸŽ¯ ConfiguraciÃ³n completada - Solo librerÃ­as compatibles!\n",
      "ðŸ’¡ Usando implementaciÃ³n propia sin dependencias problemÃ¡ticas\n"
     ]
    }
   ],
   "source": [
    "# âœ… CONFIGURACIÃ“N SIMPLIFICADA - SOLO LIBRERÃAS COMPATIBLES\n",
    "# Usando Ãºnicamente las librerÃ­as que funcionan con Python 3.13\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ“š Importando solo librerÃ­as compatibles...\")\n",
    "\n",
    "# Importaciones bÃ¡sicas que SÃ funcionan\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "print(\"âœ… LibrerÃ­as bÃ¡sicas importadas correctamente\")\n",
    "print(f\"   - NumPy: {np.__version__}\")\n",
    "\n",
    "# Verificar PyTorch (compatible)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "\n",
    "    # Configurar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ðŸ”§ Dispositivo configurado: {device}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"ðŸ’» Usando CPU\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error importando PyTorch: {e}\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Crear un tokenizador MANUAL sin Transformers\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Tokenizador simple para generar texto sin dependencias externas\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"Entrenar el tokenizador con el texto\"\"\"\n",
    "        chars = sorted(set(text))\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        \n",
    "        print(f\"âœ… Tokenizador entrenado: {self.vocab_size} caracteres Ãºnicos\")\n",
    "        return self\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convertir texto a secuencia de nÃºmeros\"\"\"\n",
    "        return [self.char_to_idx.get(ch, 0) for ch in text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Convertir secuencia de nÃºmeros a texto\"\"\"\n",
    "        return ''.join([self.idx_to_char.get(token, '?') for token in tokens])\n",
    "\n",
    "# Modelo LSTM simple usando solo PyTorch\n",
    "class SimpleLSTMGenerator(nn.Module):\n",
    "    \"\"\"Modelo LSTM simple para generar texto\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super(SimpleLSTMGenerator, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "print(\"âœ… Clases personalizadas definidas\")\n",
    "\n",
    "# Dataset personalizado\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para secuencias de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, text, tokenizer, seq_length=50):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = torch.tensor(self.tokens[idx:idx + self.seq_length], dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.tokens[idx + 1:idx + self.seq_length + 1], dtype=torch.long)\n",
    "        return input_seq, target_seq\n",
    "\n",
    "print(\"âœ… Dataset personalizado definido\")\n",
    "\n",
    "# FunciÃ³n de generaciÃ³n de texto\n",
    "def generate_text(model, tokenizer, seed_text, max_length=200, temperature=0.8):\n",
    "    \"\"\"Generar texto usando el modelo entrenado\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Preparar entrada inicial\n",
    "        current_seq = tokenizer.encode(seed_text)\n",
    "        generated = current_seq.copy()\n",
    "        \n",
    "        hidden = None\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Convertir a tensor\n",
    "            input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
    "            \n",
    "            # PredicciÃ³n\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # Aplicar temperatura y seleccionar siguiente token\n",
    "            logits = output[0, -1] / temperature\n",
    "            probabilities = torch.softmax(logits, dim=0)\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            current_seq = current_seq[1:] + [next_token]\n",
    "            \n",
    "            # Parar si se encuentra un token de fin (opcional)\n",
    "            if next_token == 0:  # Asumiendo que 0 es un token especial\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "print(\"âœ… FunciÃ³n de generaciÃ³n definida\")\n",
    "\n",
    "# Configurar reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"\\nðŸŽ¯ ConfiguraciÃ³n completada - Solo librerÃ­as compatibles!\")\n",
    "print(\"ðŸ’¡ Usando implementaciÃ³n propia sin dependencias problemÃ¡ticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165b657",
   "metadata": {
    "id": "a165b657"
   },
   "source": [
    "# ðŸ“Š ETAPA 2: PREPROCESAMIENTO Y ANÃLISIS EXPLORATORIO\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd5d9906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5d9906",
    "outputId": "7f4135ad-641d-4f18-9cf7-7cfc24b32ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado: Romeo and Juliet - 147743 caracteres\n",
      "Descargado: Hamlet - 184685 caracteres\n",
      "Descargado: Macbeth - 108654 caracteres\n",
      "Descargado: The Tempest - 102608 caracteres\n",
      "Descargado: A Midsummer Night's Dream - 227722 caracteres\n",
      "\n",
      "Texto combinado: 771420 caracteres\n",
      "Primeros 500 caracteres:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE TRAGEDY OF ROMEO AND JULIET\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "THE PROLOGUE.\n",
      "\n",
      "ACT I\n",
      "Scene I. A public place.\n",
      "Scene II. A Street.\n",
      "Scene III. Room in Capuletâ€™s House.\n",
      "Scene IV. A Street.\n",
      "Scene V. A Hall in Capuletâ€™s House.\n",
      "\n",
      "ACT II\n",
      "CHORUS.\n",
      "Scene I. An open place adjoining Capuletâ€™s Garden.\n",
      "Scene II. Capuletâ€™s Garden.\n",
      "Scene III. Friar Lawrenceâ€™s Cell.\n",
      "Scene IV. A Street.\n",
      "Scene V. Capuletâ€™s Garden.\n",
      "Scene VI. Friar Lawrenceâ€™s Cell.\n",
      "\n",
      "ACT III\n",
      "Scene I. A public ...\n"
     ]
    }
   ],
   "source": [
    "# Descargar textos de Project Gutenberg\n",
    "def download_gutenberg_text(book_id, title):\n",
    "    \"\"\"Descargar libro de Project Gutenberg\"\"\"\n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Limpiar texto\n",
    "        text = response.text\n",
    "\n",
    "        # Encontrar inicio y fin del texto principal\n",
    "        start_markers = [\"*** START OF\", \"***START OF\"]\n",
    "        end_markers = [\"*** END OF\", \"***END OF\"]\n",
    "\n",
    "        start_idx = 0\n",
    "        for marker in start_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                start_idx = text.find('\\n', idx) + 1\n",
    "                break\n",
    "\n",
    "        end_idx = len(text)\n",
    "        for marker in end_markers:\n",
    "            idx = text.find(marker)\n",
    "            if idx != -1:\n",
    "                end_idx = idx\n",
    "                break\n",
    "\n",
    "        text = text[start_idx:end_idx]\n",
    "\n",
    "        # Guardar archivo\n",
    "        filename = f\"{title.replace(' ', '_').lower()}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "        print(f\"Descargado: {title} - {len(text)} caracteres\")\n",
    "        return filename, text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {title}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Lista de libros para descargar\n",
    "books = [\n",
    "    (1513, \"Romeo and Juliet\"),\n",
    "    (1524, \"Hamlet\"),\n",
    "    (1533, \"Macbeth\"),\n",
    "    (1540, \"The Tempest\"),\n",
    "    (23, \"A Midsummer Night's Dream\")\n",
    "]\n",
    "\n",
    "# Descargar todos los libros\n",
    "all_texts = []\n",
    "for book_id, title in books:\n",
    "    filename, text = download_gutenberg_text(book_id, title)\n",
    "    if text:\n",
    "        all_texts.append(text)\n",
    "\n",
    "# Combinar todos los textos\n",
    "combined_text = '\\n\\n'.join(all_texts)\n",
    "print(f\"\\nTexto combinado: {len(combined_text)} caracteres\")\n",
    "print(f\"Primeros 500 caracteres:\\n{combined_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0ec157f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "090a89e72dcd462291ba212c77c91b41",
      "aedeaa5e902749c59f88acf9fdd54d36",
      "6af00cf46bed4e19bfddb1d13ec9c9b5",
      "640dbb9c803c41dfb4a6c44cf8de9efa",
      "5f3935f30db84f4eae7a6bd9cf76abf2",
      "e8e862835f424d4eb8c41c835c9de1d0",
      "67cfbb63ff90404baef31a360cb3e241",
      "bd3a4f11fc6f4db380207d8fc9ea74e3",
      "dcbae98fc69b4ff5afa41b758cfe6f94",
      "cd9624e2dcf04de89253512a87d75908",
      "715e3f4a421b48729ebdf13094ed1f45",
      "5151520b211548e6bada1b8adca6181a",
      "9aafc26f35794386a4208c866b2a1757",
      "cdee77fb9e474375a1ab4e20789072a1",
      "780f7b39dcab4395b89754a39d550f0f",
      "ca4e16ff781a49d6afd55808c9ecc245",
      "692f4311bc9f40faae6b3dfb828850c2",
      "fdc178cb5f84479a8ef2fec428beffd4",
      "aece639a78d64761be478c2808f63d01",
      "576acb57204048429785ed022fc747b4",
      "c62a7c0aa9c64381b01ada5ca3e17362",
      "ea4e95e55f6a4ca0a9dd722ce7852593",
      "f7d8839302a0464eb8292fe7e0768e81",
      "4363682f26a647bb8c1047f00ab6729a",
      "88363a8a114a4f0f9b553854dac0e0d0",
      "c9b59e54ab614684befc438ea007ac81",
      "d15d845f936a45df935b46e04c4cbfd5",
      "6d1ad811d2ef419bbc1dbf78b647d40f",
      "11f349bc5c0045bb94a702d61f176a07",
      "4a362d33e41b4cc0be3435f81488bef7",
      "114a935c3989491a9aca1263a6fa68c9",
      "22ed64a40bee41828ac80666dffb7895",
      "0ac8a6e43caf4e6596f1c03c428f615e",
      "a6432e9a307942438e57338b5e0047db",
      "6be7af639706460896c77088c6e0f8a9",
      "dfe74e9a22ff4bfeb1ad47e4e713a2ef",
      "d08d421b9b584cbdbd77d66406cdd6f6",
      "6da9b477c5e646baa30238e5f059a155",
      "1edce24cb707493fa2f69febb11c6444",
      "2101f4478de74d21b1d13600f2530b8a",
      "e108978b57fd4e43a31de2ef9fa6e695",
      "3328d4c6ba744920a412f60ec9190024",
      "e8c600c418bc4845b2e189e345ec9812",
      "d124eab457ed4fd09a79ebb71a80cf34",
      "f83bf28594d14387989ae0a9563ba464",
      "dbe6ab6b9b9a475fb9831a3270b68ca4",
      "3ed9c42960e3411a910364857d73d3d1",
      "2ca6f0b2f1fa4a6da44b73c0066a0a4f",
      "61f1aab215434dffaeb64d1c7d0632da",
      "ffa6d136227d4562a6008babe4a710b7",
      "ac08b0e1d4b64ddbb4fc87abafb4a027",
      "6345c42875c14f428d979f5d6c45b222",
      "98e0c4e7c6224f13bc87f884de436923",
      "70ebe4f77c0346d99a005ef448caf386",
      "bda0f6c908cc4ba1959ebcfccc29f618"
     ]
    },
    "id": "c0ec157f",
    "outputId": "00db1ce6-9267-4244-de86-f9db6bbb2f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando texto para fine-tuning de GPT-2...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2154\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2155\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2184\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2182\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:63\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeam_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcandidate_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     64\u001b[39m     AssistantVocabTranslatorCache,\n\u001b[32m     65\u001b[39m     AssistedCandidateGenerator,\n\u001b[32m     66\u001b[39m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[32m     67\u001b[39m     CandidateGenerator,\n\u001b[32m     68\u001b[39m     EarlyExitCandidateGenerator,\n\u001b[32m     69\u001b[39m     PromptLookupCandidateGenerator,\n\u001b[32m     70\u001b[39m     UniversalSpeculativeDecodingGenerator,\n\u001b[32m     71\u001b[39m     _crop_past_key_values,\n\u001b[32m     72\u001b[39m     _prepare_attention_mask,\n\u001b[32m     73\u001b[39m     _prepare_token_type_ids,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     76\u001b[39m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[32m     77\u001b[39m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     GenerationMode,\n\u001b[32m     81\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py:29\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     81\u001b[39m     __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     82\u001b[39m     _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\__init__.py:300\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_base.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      6\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      7\u001b[39m                        matrix, validateaxis, getdtype)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\_util.py:13\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\_array_api.py:18\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py:392\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings(record=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[32m    393\u001b[39m     _mac_os_check()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy.strings'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparando texto para fine-tuning de GPT-2...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Inicializar tokenizador de GPT-2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mGPT2Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# AÃ±adir token de padding si no existe\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2012\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2014\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2133\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2129\u001b[39m     config = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2131\u001b[39m     \u001b[38;5;66;03m# Third attempt. If we have not yet found the original type of the tokenizer,\u001b[39;00m\n\u001b[32m   2132\u001b[39m     \u001b[38;5;66;03m# we are loading we see if we can infer it from the type of the configuration file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2133\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_MAPPING_NAMES  \u001b[38;5;66;03m# tests_ignore\u001b[39;00m\n\u001b[32m   2135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2136\u001b[39m         model_type = config.model_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:38\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     cached_file,\n\u001b[32m     31\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     logging,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     41\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2155\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2158\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2159\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   2162\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "print(\"Preparando texto para fine-tuning de GPT-2...\")\n",
    "\n",
    "# Inicializar tokenizador de GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# AÃ±adir token de padding si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizador cargado: {tokenizer.name_or_path}\")\n",
    "print(f\"Vocabulario: {len(tokenizer)} tokens\")\n",
    "\n",
    "# Limpiar y preparar texto\n",
    "def clean_text_for_gpt2(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\\"\\'\\(\\)]', '', text)\n",
    "    text = re.sub(r'\\s+([\\.,:;!?])', r'\\1', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ðŸ‘‰ AsegÃºrate de definir esta variable:\n",
    "# combined_text = \"Tu texto aquÃ­...\"\n",
    "\n",
    "clean_combined_text = clean_text_for_gpt2(combined_text)\n",
    "\n",
    "# Tokenizar texto\n",
    "print(\"Tokenizando texto...\")\n",
    "tokens = tokenizer.encode(clean_combined_text)\n",
    "print(f\"Texto tokenizado: {len(tokens)} tokens\")\n",
    "\n",
    "def create_text_chunks(tokens, chunk_size=512, overlap=50):\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - chunk_size, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "CHUNK_SIZE = 256\n",
    "chunks = create_text_chunks(tokens, CHUNK_SIZE)\n",
    "print(f\"Creados {len(chunks)} chunks de texto para entrenamiento\")\n",
    "\n",
    "print(\"\\n=== EJEMPLOS DE TEXTO PROCESADO ===\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    decoded = tokenizer.decode(chunks[i][:50])\n",
    "    print(f\"Chunk {i+1}: {decoded}...\")\n",
    "\n",
    "preprocessed_data = {\n",
    "    'chunks': chunks,\n",
    "    'tokenizer_name': 'gpt2',\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'total_tokens': len(tokens)\n",
    "}\n",
    "\n",
    "with open('preprocessed_shakespeare.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Datos preprocessados guardados exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1696874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Preparando texto con tokenizador compatible...\n",
      "ðŸ“„ Texto disponible: 771420 caracteres\n",
      "âœ… Texto limpio: 734945 caracteres\n",
      "âœ… Tokenizador entrenado: 46 caracteres Ãºnicos\n",
      "ðŸ“ Vocabulario: 46 caracteres Ãºnicos\n",
      "   Caracteres: [' ', '!', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?']...\n",
      "ðŸ”  Texto tokenizado: 734945 tokens\n",
      "ðŸ“¦ Creados 9798 chunks de entrenamiento\n",
      "\n",
      "=== EJEMPLOS DE TEXTO PROCESADO ===\n",
      "Chunk 1: the tragedy of romeo and julie...\n",
      "Chunk 2: e. act i scene i. a public pla...\n",
      "Chunk 3: pulets house. scene iv. a stre...\n",
      "ðŸ’¾ Datos preprocessados guardados en 'preprocessed_simple.pkl'\n",
      "âœ… Preprocesamiento completado con Ã©xito\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ PROCESAMIENTO DE TEXTO - SOLO LIBRERÃAS COMPATIBLES\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "print(\"ðŸ”§ Preparando texto con tokenizador compatible...\")\n",
    "\n",
    "# Verificar que tenemos texto de la celda anterior\n",
    "if 'combined_text' not in locals():\n",
    "    print(\"âš ï¸ No se encuentra 'combined_text'. Usando texto de ejemplo...\")\n",
    "    combined_text = \"\"\"\n",
    "    To be or not to be, that is the question.\n",
    "    Whether 'tis nobler in the mind to suffer\n",
    "    The slings and arrows of outrageous fortune,\n",
    "    Or to take arms against a sea of troubles\n",
    "    And by opposing end them.\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"ðŸ“„ Texto disponible: {len(combined_text)} caracteres\")\n",
    "\n",
    "# Limpiar texto\n",
    "def clean_text_simple(text):\n",
    "    \"\"\"Limpiar texto manteniendo solo caracteres importantes\"\"\"\n",
    "    # Convertir a minÃºsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Mantener solo letras, nÃºmeros, espacios y puntuaciÃ³n bÃ¡sica\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:\\-\\\"\\'\\(\\)\\n]', '', text)\n",
    "    \n",
    "    # Normalizar espacios\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Limpiar lÃ­neas vacÃ­as mÃºltiples\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "clean_combined_text = clean_text_simple(combined_text)\n",
    "print(f\"âœ… Texto limpio: {len(clean_combined_text)} caracteres\")\n",
    "\n",
    "# Crear y entrenar tokenizador simple\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(clean_combined_text)\n",
    "\n",
    "print(f\"ðŸ“ Vocabulario: {tokenizer.vocab_size} caracteres Ãºnicos\")\n",
    "print(f\"   Caracteres: {list(tokenizer.char_to_idx.keys())[:20]}...\")\n",
    "\n",
    "# Tokenizar texto completo\n",
    "tokens = tokenizer.encode(clean_combined_text)\n",
    "print(f\"ðŸ”  Texto tokenizado: {len(tokens)} tokens\")\n",
    "\n",
    "# Crear chunks para entrenamiento\n",
    "def create_text_chunks(tokens, chunk_size=100, overlap=25):\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - chunk_size, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "CHUNK_SIZE = 100\n",
    "chunks = create_text_chunks(tokens, CHUNK_SIZE)\n",
    "print(f\"ðŸ“¦ Creados {len(chunks)} chunks de entrenamiento\")\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\n=== EJEMPLOS DE TEXTO PROCESADO ===\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    if len(chunks[i]) >= 30:\n",
    "        decoded = tokenizer.decode(chunks[i][:30])\n",
    "        print(f\"Chunk {i+1}: {decoded}...\")\n",
    "\n",
    "# Guardar datos preprocessados\n",
    "preprocessed_data = {\n",
    "    'chunks': chunks,\n",
    "    'tokenizer': tokenizer,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'total_tokens': len(tokens),\n",
    "    'clean_text': clean_combined_text[:1000]  # Solo primeros 1000 chars para referencia\n",
    "}\n",
    "\n",
    "with open('preprocessed_simple.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"ðŸ’¾ Datos preprocessados guardados en 'preprocessed_simple.pkl'\")\n",
    "print(\"âœ… Preprocesamiento completado con Ã©xito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6db7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ ENTRENAMIENTO DEL MODELO - SOLO LIBRERÃAS COMPATIBLES\n",
      "âœ… Dispositivo: cuda\n",
      "ðŸ“„ Datos cargados: 9798 chunks, vocab_size: 46\n",
      "ðŸ“¦ Dataset creado: 979770 secuencias\n",
      "ðŸ—ï¸ Modelo creado con 939310 parÃ¡metros\n",
      "\n",
      "ðŸš€ Iniciando entrenamiento por 3 Ã©pocas...\n",
      "\n",
      "=== Ã‰POCA 1/3 ===\n",
      "  Batch 0/30618, Loss: 3.8184\n",
      "  Batch 20/30618, Loss: 2.9232\n",
      "  Batch 40/30618, Loss: 2.7099\n",
      "  Batch 60/30618, Loss: 2.5414\n",
      "  Batch 80/30618, Loss: 2.3422\n",
      "  Batch 100/30618, Loss: 2.3981\n",
      "  Batch 120/30618, Loss: 2.2520\n",
      "  Batch 140/30618, Loss: 2.2121\n",
      "  Batch 160/30618, Loss: 2.1975\n",
      "  Batch 180/30618, Loss: 2.1668\n",
      "  Batch 200/30618, Loss: 2.1695\n",
      "  Batch 220/30618, Loss: 2.0791\n",
      "  Batch 240/30618, Loss: 2.1247\n",
      "  Batch 260/30618, Loss: 2.0652\n",
      "  Batch 280/30618, Loss: 2.0954\n",
      "  Batch 300/30618, Loss: 1.9804\n",
      "  Batch 320/30618, Loss: 2.0001\n",
      "  Batch 340/30618, Loss: 1.9942\n",
      "  Batch 360/30618, Loss: 2.0011\n",
      "  Batch 380/30618, Loss: 1.9962\n",
      "  Batch 400/30618, Loss: 2.0606\n",
      "  Batch 420/30618, Loss: 1.8563\n",
      "  Batch 440/30618, Loss: 1.9119\n",
      "  Batch 460/30618, Loss: 1.9141\n",
      "  Batch 480/30618, Loss: 1.8751\n",
      "  Batch 500/30618, Loss: 1.9603\n",
      "  Batch 520/30618, Loss: 1.8987\n",
      "  Batch 540/30618, Loss: 1.8787\n",
      "  Batch 560/30618, Loss: 1.8352\n",
      "  Batch 580/30618, Loss: 1.8671\n",
      "  Batch 600/30618, Loss: 1.9523\n",
      "  Batch 620/30618, Loss: 1.8419\n",
      "  Batch 640/30618, Loss: 1.8604\n",
      "  Batch 660/30618, Loss: 1.7990\n",
      "  Batch 680/30618, Loss: 1.8712\n",
      "  Batch 700/30618, Loss: 1.8520\n",
      "  Batch 720/30618, Loss: 1.8038\n",
      "  Batch 740/30618, Loss: 1.8023\n",
      "  Batch 760/30618, Loss: 1.7617\n",
      "  Batch 780/30618, Loss: 1.8002\n",
      "  Batch 800/30618, Loss: 1.7788\n",
      "  Batch 820/30618, Loss: 1.9156\n",
      "  Batch 840/30618, Loss: 1.7987\n",
      "  Batch 860/30618, Loss: 1.8583\n",
      "  Batch 880/30618, Loss: 1.8481\n",
      "  Batch 900/30618, Loss: 1.7752\n",
      "  Batch 920/30618, Loss: 1.7432\n",
      "  Batch 940/30618, Loss: 1.8255\n",
      "  Batch 960/30618, Loss: 1.9017\n",
      "  Batch 980/30618, Loss: 1.7416\n",
      "  Batch 1000/30618, Loss: 1.7497\n",
      "  Batch 1020/30618, Loss: 1.8247\n",
      "  Batch 1040/30618, Loss: 1.7070\n",
      "  Batch 1060/30618, Loss: 1.7639\n",
      "  Batch 1080/30618, Loss: 1.7469\n",
      "  Batch 1100/30618, Loss: 1.8685\n",
      "  Batch 1120/30618, Loss: 1.7441\n",
      "  Batch 1140/30618, Loss: 1.7440\n",
      "  Batch 1160/30618, Loss: 1.7922\n",
      "  Batch 1180/30618, Loss: 1.7485\n",
      "  Batch 1200/30618, Loss: 1.7158\n",
      "  Batch 1220/30618, Loss: 1.6741\n",
      "  Batch 1240/30618, Loss: 1.6747\n",
      "  Batch 1260/30618, Loss: 1.7812\n",
      "  Batch 1280/30618, Loss: 1.7128\n",
      "  Batch 1300/30618, Loss: 1.6734\n",
      "  Batch 1320/30618, Loss: 1.7783\n",
      "  Batch 1340/30618, Loss: 1.7319\n",
      "  Batch 1360/30618, Loss: 1.6867\n",
      "  Batch 1380/30618, Loss: 1.7670\n",
      "  Batch 1400/30618, Loss: 1.7566\n",
      "  Batch 1420/30618, Loss: 1.7214\n",
      "  Batch 1440/30618, Loss: 1.6815\n",
      "  Batch 1460/30618, Loss: 1.7077\n",
      "  Batch 1480/30618, Loss: 1.6933\n",
      "  Batch 1500/30618, Loss: 1.6960\n",
      "  Batch 1520/30618, Loss: 1.7348\n",
      "  Batch 1540/30618, Loss: 1.6501\n",
      "  Batch 1560/30618, Loss: 1.5934\n",
      "  Batch 1580/30618, Loss: 1.6211\n",
      "  Batch 1600/30618, Loss: 1.7086\n",
      "  Batch 1620/30618, Loss: 1.7907\n",
      "  Batch 1640/30618, Loss: 1.7184\n",
      "  Batch 1660/30618, Loss: 1.6228\n",
      "  Batch 1680/30618, Loss: 1.6128\n",
      "  Batch 1700/30618, Loss: 1.6914\n",
      "  Batch 1720/30618, Loss: 1.6399\n",
      "  Batch 1740/30618, Loss: 1.6217\n",
      "  Batch 1760/30618, Loss: 1.6691\n",
      "  Batch 1780/30618, Loss: 1.6417\n",
      "  Batch 1800/30618, Loss: 1.6803\n",
      "  Batch 1820/30618, Loss: 1.6288\n",
      "  Batch 1840/30618, Loss: 1.6842\n",
      "  Batch 1860/30618, Loss: 1.5673\n",
      "  Batch 1880/30618, Loss: 1.7280\n",
      "  Batch 1900/30618, Loss: 1.6376\n",
      "  Batch 1920/30618, Loss: 1.5881\n",
      "  Batch 1940/30618, Loss: 1.6624\n",
      "  Batch 1960/30618, Loss: 1.7354\n",
      "  Batch 1980/30618, Loss: 1.7014\n",
      "  Batch 2000/30618, Loss: 1.7196\n",
      "  Batch 2020/30618, Loss: 1.6100\n",
      "  Batch 2040/30618, Loss: 1.6956\n",
      "  Batch 2060/30618, Loss: 1.6120\n",
      "  Batch 2080/30618, Loss: 1.6140\n",
      "  Batch 2100/30618, Loss: 1.6770\n",
      "  Batch 2120/30618, Loss: 1.5546\n",
      "  Batch 2140/30618, Loss: 1.6867\n",
      "  Batch 2160/30618, Loss: 1.6184\n",
      "  Batch 2180/30618, Loss: 1.6053\n",
      "  Batch 2200/30618, Loss: 1.5797\n",
      "  Batch 2220/30618, Loss: 1.6284\n",
      "  Batch 2240/30618, Loss: 1.6124\n",
      "  Batch 2260/30618, Loss: 1.6247\n",
      "  Batch 2280/30618, Loss: 1.6623\n",
      "  Batch 2300/30618, Loss: 1.5874\n",
      "  Batch 2320/30618, Loss: 1.6320\n",
      "  Batch 2340/30618, Loss: 1.6082\n",
      "  Batch 2360/30618, Loss: 1.5967\n",
      "  Batch 2380/30618, Loss: 1.6278\n",
      "  Batch 2400/30618, Loss: 1.5586\n",
      "  Batch 2420/30618, Loss: 1.6250\n",
      "  Batch 2440/30618, Loss: 1.5769\n",
      "  Batch 2460/30618, Loss: 1.6669\n",
      "  Batch 2480/30618, Loss: 1.5742\n",
      "  Batch 2500/30618, Loss: 1.5967\n",
      "  Batch 2520/30618, Loss: 1.6425\n",
      "  Batch 2540/30618, Loss: 1.5547\n",
      "  Batch 2560/30618, Loss: 1.6023\n",
      "  Batch 2580/30618, Loss: 1.6405\n",
      "  Batch 2600/30618, Loss: 1.5322\n",
      "  Batch 2620/30618, Loss: 1.5970\n",
      "  Batch 2640/30618, Loss: 1.6533\n",
      "  Batch 2660/30618, Loss: 1.6168\n",
      "  Batch 2680/30618, Loss: 1.6871\n",
      "  Batch 2700/30618, Loss: 1.5394\n",
      "  Batch 2720/30618, Loss: 1.5971\n",
      "  Batch 2740/30618, Loss: 1.5481\n",
      "  Batch 2760/30618, Loss: 1.5786\n",
      "  Batch 2780/30618, Loss: 1.6296\n",
      "  Batch 2800/30618, Loss: 1.5940\n",
      "  Batch 2820/30618, Loss: 1.6245\n",
      "  Batch 2840/30618, Loss: 1.5819\n",
      "  Batch 2860/30618, Loss: 1.5469\n",
      "  Batch 2880/30618, Loss: 1.5437\n",
      "  Batch 2900/30618, Loss: 1.5446\n",
      "  Batch 2920/30618, Loss: 1.5829\n",
      "  Batch 2940/30618, Loss: 1.6220\n",
      "  Batch 2960/30618, Loss: 1.6269\n",
      "  Batch 2980/30618, Loss: 1.5202\n",
      "  Batch 3000/30618, Loss: 1.5975\n",
      "  Batch 3020/30618, Loss: 1.6709\n",
      "  Batch 3040/30618, Loss: 1.6567\n",
      "  Batch 3060/30618, Loss: 1.6110\n",
      "  Batch 3080/30618, Loss: 1.5078\n",
      "  Batch 3100/30618, Loss: 1.5229\n",
      "  Batch 3120/30618, Loss: 1.6136\n",
      "  Batch 3140/30618, Loss: 1.5114\n",
      "  Batch 3160/30618, Loss: 1.5336\n",
      "  Batch 3180/30618, Loss: 1.6220\n",
      "  Batch 3200/30618, Loss: 1.5736\n",
      "  Batch 3220/30618, Loss: 1.5838\n",
      "  Batch 3240/30618, Loss: 1.5341\n",
      "  Batch 3260/30618, Loss: 1.5709\n",
      "  Batch 3280/30618, Loss: 1.5746\n",
      "  Batch 3300/30618, Loss: 1.5329\n",
      "  Batch 3320/30618, Loss: 1.6058\n",
      "  Batch 3340/30618, Loss: 1.6696\n",
      "  Batch 3360/30618, Loss: 1.4875\n",
      "  Batch 3380/30618, Loss: 1.6460\n",
      "  Batch 3400/30618, Loss: 1.5640\n",
      "  Batch 3420/30618, Loss: 1.5703\n",
      "  Batch 3440/30618, Loss: 1.5806\n",
      "  Batch 3460/30618, Loss: 1.5982\n",
      "  Batch 3480/30618, Loss: 1.4847\n",
      "  Batch 3500/30618, Loss: 1.4851\n",
      "  Batch 3520/30618, Loss: 1.5601\n",
      "  Batch 3540/30618, Loss: 1.6124\n",
      "  Batch 3560/30618, Loss: 1.5119\n",
      "  Batch 3580/30618, Loss: 1.5431\n",
      "  Batch 3600/30618, Loss: 1.5856\n",
      "  Batch 3620/30618, Loss: 1.5922\n",
      "  Batch 3640/30618, Loss: 1.4953\n",
      "  Batch 3660/30618, Loss: 1.5783\n",
      "  Batch 3680/30618, Loss: 1.5063\n",
      "  Batch 3700/30618, Loss: 1.4833\n",
      "  Batch 3720/30618, Loss: 1.5778\n",
      "  Batch 3740/30618, Loss: 1.5251\n",
      "  Batch 3760/30618, Loss: 1.5966\n",
      "  Batch 3780/30618, Loss: 1.5015\n",
      "  Batch 3800/30618, Loss: 1.5577\n",
      "  Batch 3820/30618, Loss: 1.5524\n",
      "  Batch 3840/30618, Loss: 1.5142\n",
      "  Batch 3860/30618, Loss: 1.5880\n",
      "  Batch 3880/30618, Loss: 1.6036\n",
      "  Batch 3900/30618, Loss: 1.5336\n",
      "  Batch 3920/30618, Loss: 1.5901\n",
      "  Batch 3940/30618, Loss: 1.5331\n",
      "  Batch 3960/30618, Loss: 1.5636\n",
      "  Batch 3980/30618, Loss: 1.6296\n",
      "  Batch 4000/30618, Loss: 1.5623\n",
      "  Batch 4020/30618, Loss: 1.5413\n",
      "  Batch 4040/30618, Loss: 1.5160\n",
      "  Batch 4060/30618, Loss: 1.5824\n",
      "  Batch 4080/30618, Loss: 1.5100\n",
      "  Batch 4100/30618, Loss: 1.6103\n",
      "  Batch 4120/30618, Loss: 1.5527\n",
      "  Batch 4140/30618, Loss: 1.5767\n",
      "  Batch 4160/30618, Loss: 1.5625\n",
      "  Batch 4180/30618, Loss: 1.5866\n",
      "  Batch 4200/30618, Loss: 1.6269\n",
      "  Batch 4220/30618, Loss: 1.5323\n",
      "  Batch 4240/30618, Loss: 1.5208\n",
      "  Batch 4260/30618, Loss: 1.5858\n",
      "  Batch 4280/30618, Loss: 1.6024\n",
      "  Batch 4300/30618, Loss: 1.5490\n",
      "  Batch 4320/30618, Loss: 1.4978\n",
      "  Batch 4340/30618, Loss: 1.5543\n",
      "  Batch 4360/30618, Loss: 1.5559\n",
      "  Batch 4380/30618, Loss: 1.5240\n",
      "  Batch 4400/30618, Loss: 1.5152\n",
      "  Batch 4420/30618, Loss: 1.5952\n",
      "  Batch 4440/30618, Loss: 1.6100\n",
      "  Batch 4460/30618, Loss: 1.5095\n",
      "  Batch 4480/30618, Loss: 1.5357\n",
      "  Batch 4500/30618, Loss: 1.5716\n",
      "  Batch 4520/30618, Loss: 1.5420\n",
      "  Batch 4540/30618, Loss: 1.6400\n",
      "  Batch 4560/30618, Loss: 1.4800\n",
      "  Batch 4580/30618, Loss: 1.5913\n",
      "  Batch 4600/30618, Loss: 1.4906\n",
      "  Batch 4620/30618, Loss: 1.6171\n",
      "  Batch 4640/30618, Loss: 1.5497\n",
      "  Batch 4660/30618, Loss: 1.4614\n",
      "  Batch 4680/30618, Loss: 1.5531\n",
      "  Batch 4700/30618, Loss: 1.4367\n",
      "  Batch 4720/30618, Loss: 1.5266\n",
      "  Batch 4740/30618, Loss: 1.5052\n",
      "  Batch 4760/30618, Loss: 1.5738\n",
      "  Batch 4780/30618, Loss: 1.4808\n",
      "  Batch 4800/30618, Loss: 1.5258\n",
      "  Batch 4820/30618, Loss: 1.4963\n",
      "  Batch 4840/30618, Loss: 1.5676\n",
      "  Batch 4860/30618, Loss: 1.4516\n",
      "  Batch 4880/30618, Loss: 1.5474\n",
      "  Batch 4900/30618, Loss: 1.5915\n",
      "  Batch 4920/30618, Loss: 1.5432\n",
      "  Batch 4940/30618, Loss: 1.4229\n",
      "  Batch 4960/30618, Loss: 1.4873\n",
      "  Batch 4980/30618, Loss: 1.5078\n",
      "  Batch 5000/30618, Loss: 1.5352\n",
      "  Batch 5020/30618, Loss: 1.5985\n",
      "  Batch 5040/30618, Loss: 1.5192\n",
      "  Batch 5060/30618, Loss: 1.4658\n",
      "  Batch 5080/30618, Loss: 1.5162\n",
      "  Batch 5100/30618, Loss: 1.4353\n",
      "  Batch 5120/30618, Loss: 1.4891\n",
      "  Batch 5140/30618, Loss: 1.5425\n",
      "  Batch 5160/30618, Loss: 1.4861\n",
      "  Batch 5180/30618, Loss: 1.4000\n",
      "  Batch 5200/30618, Loss: 1.5241\n",
      "  Batch 5220/30618, Loss: 1.4537\n",
      "  Batch 5240/30618, Loss: 1.4957\n",
      "  Batch 5260/30618, Loss: 1.4564\n",
      "  Batch 5280/30618, Loss: 1.4491\n",
      "  Batch 5300/30618, Loss: 1.4997\n",
      "  Batch 5320/30618, Loss: 1.5133\n",
      "  Batch 5340/30618, Loss: 1.5363\n",
      "  Batch 5360/30618, Loss: 1.5009\n",
      "  Batch 5380/30618, Loss: 1.4409\n",
      "  Batch 5400/30618, Loss: 1.4806\n",
      "  Batch 5420/30618, Loss: 1.5733\n",
      "  Batch 5440/30618, Loss: 1.5067\n",
      "  Batch 5460/30618, Loss: 1.5192\n",
      "  Batch 5480/30618, Loss: 1.4812\n",
      "  Batch 5500/30618, Loss: 1.5579\n",
      "  Batch 5520/30618, Loss: 1.4652\n",
      "  Batch 5540/30618, Loss: 1.4692\n",
      "  Batch 5560/30618, Loss: 1.5777\n",
      "  Batch 5580/30618, Loss: 1.5442\n",
      "  Batch 5600/30618, Loss: 1.5344\n",
      "  Batch 5620/30618, Loss: 1.5480\n",
      "  Batch 5640/30618, Loss: 1.5928\n",
      "  Batch 5660/30618, Loss: 1.5574\n",
      "  Batch 5680/30618, Loss: 1.6067\n",
      "  Batch 5700/30618, Loss: 1.4638\n",
      "  Batch 5720/30618, Loss: 1.4902\n",
      "  Batch 5740/30618, Loss: 1.4904\n",
      "  Batch 5760/30618, Loss: 1.5020\n",
      "  Batch 5780/30618, Loss: 1.5418\n",
      "  Batch 5800/30618, Loss: 1.5424\n",
      "  Batch 5820/30618, Loss: 1.5464\n",
      "  Batch 5840/30618, Loss: 1.5028\n",
      "  Batch 5860/30618, Loss: 1.4844\n",
      "  Batch 5880/30618, Loss: 1.4859\n",
      "  Batch 5900/30618, Loss: 1.5563\n",
      "  Batch 5920/30618, Loss: 1.5028\n",
      "  Batch 5940/30618, Loss: 1.5363\n",
      "  Batch 5960/30618, Loss: 1.4459\n",
      "  Batch 5980/30618, Loss: 1.4081\n",
      "  Batch 6000/30618, Loss: 1.4525\n",
      "  Batch 6020/30618, Loss: 1.5825\n",
      "  Batch 6040/30618, Loss: 1.4682\n",
      "  Batch 6060/30618, Loss: 1.4935\n",
      "  Batch 6080/30618, Loss: 1.5312\n",
      "  Batch 6100/30618, Loss: 1.4358\n",
      "  Batch 6120/30618, Loss: 1.5363\n",
      "  Batch 6140/30618, Loss: 1.5322\n",
      "  Batch 6160/30618, Loss: 1.5603\n",
      "  Batch 6180/30618, Loss: 1.4441\n",
      "  Batch 6200/30618, Loss: 1.5324\n",
      "  Batch 6220/30618, Loss: 1.5307\n",
      "  Batch 6240/30618, Loss: 1.5531\n",
      "  Batch 6260/30618, Loss: 1.4648\n",
      "  Batch 6280/30618, Loss: 1.4925\n",
      "  Batch 6300/30618, Loss: 1.4962\n",
      "  Batch 6320/30618, Loss: 1.4599\n",
      "  Batch 6340/30618, Loss: 1.4664\n",
      "  Batch 6360/30618, Loss: 1.4552\n",
      "  Batch 6380/30618, Loss: 1.4685\n",
      "  Batch 6400/30618, Loss: 1.4663\n",
      "  Batch 6420/30618, Loss: 1.5484\n",
      "  Batch 6440/30618, Loss: 1.4682\n",
      "  Batch 6460/30618, Loss: 1.4218\n",
      "  Batch 6480/30618, Loss: 1.4405\n",
      "  Batch 6500/30618, Loss: 1.5112\n",
      "  Batch 6520/30618, Loss: 1.4426\n",
      "  Batch 6540/30618, Loss: 1.5138\n",
      "  Batch 6560/30618, Loss: 1.4882\n",
      "  Batch 6580/30618, Loss: 1.5044\n",
      "  Batch 6600/30618, Loss: 1.4320\n",
      "  Batch 6620/30618, Loss: 1.5176\n",
      "  Batch 6640/30618, Loss: 1.5115\n",
      "  Batch 6660/30618, Loss: 1.4675\n",
      "  Batch 6680/30618, Loss: 1.4938\n",
      "  Batch 6700/30618, Loss: 1.4778\n",
      "  Batch 6720/30618, Loss: 1.4208\n",
      "  Batch 6740/30618, Loss: 1.4640\n",
      "  Batch 6760/30618, Loss: 1.5164\n",
      "  Batch 6780/30618, Loss: 1.4886\n",
      "  Batch 6800/30618, Loss: 1.4381\n",
      "  Batch 6820/30618, Loss: 1.4838\n",
      "  Batch 6840/30618, Loss: 1.4672\n",
      "  Batch 6860/30618, Loss: 1.4139\n",
      "  Batch 6880/30618, Loss: 1.5165\n",
      "  Batch 6900/30618, Loss: 1.5201\n",
      "  Batch 6920/30618, Loss: 1.5126\n",
      "  Batch 6940/30618, Loss: 1.5358\n",
      "  Batch 6960/30618, Loss: 1.5193\n",
      "  Batch 6980/30618, Loss: 1.4051\n",
      "  Batch 7000/30618, Loss: 1.5057\n",
      "  Batch 7020/30618, Loss: 1.5173\n",
      "  Batch 7040/30618, Loss: 1.5765\n",
      "  Batch 7060/30618, Loss: 1.4476\n",
      "  Batch 7080/30618, Loss: 1.4159\n",
      "  Batch 7100/30618, Loss: 1.4757\n",
      "  Batch 7120/30618, Loss: 1.4269\n",
      "  Batch 7140/30618, Loss: 1.5556\n",
      "  Batch 7160/30618, Loss: 1.4506\n",
      "  Batch 7180/30618, Loss: 1.4994\n",
      "  Batch 7200/30618, Loss: 1.4216\n",
      "  Batch 7220/30618, Loss: 1.4868\n",
      "  Batch 7240/30618, Loss: 1.4932\n",
      "  Batch 7260/30618, Loss: 1.5135\n",
      "  Batch 7280/30618, Loss: 1.4959\n",
      "  Batch 7300/30618, Loss: 1.5340\n",
      "  Batch 7320/30618, Loss: 1.4469\n",
      "  Batch 7340/30618, Loss: 1.4146\n",
      "  Batch 7360/30618, Loss: 1.4799\n",
      "  Batch 7380/30618, Loss: 1.4775\n",
      "  Batch 7400/30618, Loss: 1.4699\n",
      "  Batch 7420/30618, Loss: 1.5048\n",
      "  Batch 7440/30618, Loss: 1.4440\n",
      "  Batch 7460/30618, Loss: 1.4974\n",
      "  Batch 7480/30618, Loss: 1.4564\n",
      "  Batch 7500/30618, Loss: 1.4700\n",
      "  Batch 7520/30618, Loss: 1.4874\n",
      "  Batch 7540/30618, Loss: 1.4540\n",
      "  Batch 7560/30618, Loss: 1.5076\n",
      "  Batch 7580/30618, Loss: 1.4620\n",
      "  Batch 7600/30618, Loss: 1.4618\n",
      "  Batch 7620/30618, Loss: 1.4248\n",
      "  Batch 7640/30618, Loss: 1.4603\n",
      "  Batch 7660/30618, Loss: 1.4923\n",
      "  Batch 7680/30618, Loss: 1.4252\n",
      "  Batch 7700/30618, Loss: 1.5276\n",
      "  Batch 7720/30618, Loss: 1.5111\n",
      "  Batch 7740/30618, Loss: 1.4635\n",
      "  Batch 7760/30618, Loss: 1.4847\n",
      "  Batch 7780/30618, Loss: 1.4131\n",
      "  Batch 7800/30618, Loss: 1.4362\n",
      "  Batch 7820/30618, Loss: 1.4674\n",
      "  Batch 7840/30618, Loss: 1.4059\n",
      "  Batch 7860/30618, Loss: 1.4570\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ ENTRENAMIENTO DEL MODELO - SOLO LIBRERÃAS COMPATIBLES\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"ðŸš€ ENTRENAMIENTO DEL MODELO - SOLO LIBRERÃAS COMPATIBLES\")\n",
    "\n",
    "# ConfiguraciÃ³n\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Dispositivo: {device}\")\n",
    "\n",
    "# Cargar datos preprocessados\n",
    "try:\n",
    "    with open('preprocessed_simple.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    tokenizer = data['tokenizer']\n",
    "    chunks = data['chunks']\n",
    "    vocab_size = data['vocab_size']\n",
    "    \n",
    "    print(f\"ðŸ“„ Datos cargados: {len(chunks)} chunks, vocab_size: {vocab_size}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ No se encontrÃ³ archivo preprocessado. Usando variables actuales...\")\n",
    "    \n",
    "    # Usar variables del kernel actual\n",
    "    if 'tokenizer' in locals() and 'chunks' in locals():\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        print(f\"ðŸ“„ Usando datos actuales: {len(chunks)} chunks, vocab_size: {vocab_size}\")\n",
    "    else:\n",
    "        print(\"âŒ No hay datos disponibles. Ejecuta primero las celdas anteriores.\")\n",
    "        raise ValueError(\"No hay datos de entrenamiento disponibles\")\n",
    "\n",
    "# Crear dataset\n",
    "SEQ_LENGTH = 30\n",
    "dataset = TextDataset(tokenizer.decode(sum(chunks, [])), tokenizer, SEQ_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"ðŸ“¦ Dataset creado: {len(dataset)} secuencias\")\n",
    "\n",
    "# Crear modelo\n",
    "model = SimpleLSTMGenerator(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=128,\n",
    "    hidden_size=256,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"ðŸ—ï¸ Modelo creado con {sum(p.numel() for p in model.parameters())} parÃ¡metros\")\n",
    "\n",
    "# Configurar entrenamiento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# FunciÃ³n de entrenamiento\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(input_seq)\n",
    "        loss = criterion(output.reshape(-1, vocab_size), target_seq.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Entrenar modelo (solo 3 Ã©pocas para prueba rÃ¡pida)\n",
    "EPOCHS = 3\n",
    "print(f\"\\nðŸš€ Iniciando entrenamiento por {EPOCHS} Ã©pocas...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n=== Ã‰POCA {epoch + 1}/{EPOCHS} ===\")\n",
    "    \n",
    "    avg_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "    print(f\"âœ… Ã‰poca {epoch + 1} completada - Loss promedio: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Generar texto de ejemplo cada Ã©poca\n",
    "    print(\"ðŸ“ Ejemplo de generaciÃ³n:\")\n",
    "    sample_text = generate_text(model, tokenizer, \"to be\", max_length=100, temperature=0.8)\n",
    "    print(f\"   '{sample_text[:100]}...'\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Entrenamiento completado!\")\n",
    "\n",
    "# Guardar modelo\n",
    "torch.save(model.state_dict(), 'simple_text_generator.pth')\n",
    "print(\"ðŸ’¾ Modelo guardado como 'simple_text_generator.pth'\")\n",
    "\n",
    "# Prueba final de generaciÃ³n\n",
    "print(\"\\n=== PRUEBA FINAL DE GENERACIÃ“N ===\")\n",
    "for seed in [\"to be\", \"the\", \"and\"]:\n",
    "    generated = generate_text(model, tokenizer, seed, max_length=150, temperature=0.7)\n",
    "    print(f\"\\nSeed: '{seed}'\")\n",
    "    print(f\"Generado: {generated[:150]}...\")\n",
    "\n",
    "print(\"\\nâœ… Â¡GeneraciÃ³n de texto completada exitosamente!\")\n",
    "print(\"ðŸ’¡ Modelo entrenado usando solo PyTorch y librerÃ­as compatibles con Python 3.13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7-oqFtkKgTzD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-oqFtkKgTzD",
    "outputId": "723ce981-986b-45bd-97cc-2fcffa7b346e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dispositivo: cuda\n",
      "â¬‡ï¸ Descargando texto de Shakespeare...\n",
      "ðŸ“„ Archivo guardado como shakespeare.txt\n",
      "ðŸ“„ Texto cargado (5378663 caracteres)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2154\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2155\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2184\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2182\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\utils.py:63\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbeam_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcandidate_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     64\u001b[39m     AssistantVocabTranslatorCache,\n\u001b[32m     65\u001b[39m     AssistedCandidateGenerator,\n\u001b[32m     66\u001b[39m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[32m     67\u001b[39m     CandidateGenerator,\n\u001b[32m     68\u001b[39m     EarlyExitCandidateGenerator,\n\u001b[32m     69\u001b[39m     PromptLookupCandidateGenerator,\n\u001b[32m     70\u001b[39m     UniversalSpeculativeDecodingGenerator,\n\u001b[32m     71\u001b[39m     _crop_past_key_values,\n\u001b[32m     72\u001b[39m     _prepare_attention_mask,\n\u001b[32m     73\u001b[39m     _prepare_token_type_ids,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     76\u001b[39m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[32m     77\u001b[39m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     GenerationMode,\n\u001b[32m     81\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py:29\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     81\u001b[39m     __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     82\u001b[39m     _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\__init__.py:300\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_base.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      6\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      7\u001b[39m                        matrix, validateaxis, getdtype)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\_util.py:13\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\_array_api.py:18\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py:392\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings(record=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[32m    393\u001b[39m     _mac_os_check()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy.strings'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m clean_combined_text = clean_text_for_gpt2(combined_text)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# === Tokenizador ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m tokenizer = \u001b[43mGPT2Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2012\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2014\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2133\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2129\u001b[39m     config = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2131\u001b[39m     \u001b[38;5;66;03m# Third attempt. If we have not yet found the original type of the tokenizer,\u001b[39;00m\n\u001b[32m   2132\u001b[39m     \u001b[38;5;66;03m# we are loading we see if we can infer it from the type of the configuration file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2133\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_MAPPING_NAMES  \u001b[38;5;66;03m# tests_ignore\u001b[39;00m\n\u001b[32m   2135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2136\u001b[39m         model_type = config.model_type\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:38\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     cached_file,\n\u001b[32m     31\u001b[39m     extract_commit_hash,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     logging,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     CONFIG_MAPPING_NAMES,\n\u001b[32m     41\u001b[39m     AutoConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     replace_list_option_in_docstrings,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cristhian Ismael\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2155\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2158\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2159\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   2162\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "import urllib.request\n",
    "\n",
    "# === ConfiguraciÃ³n ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Dispositivo: {device}\")\n",
    "\n",
    "# === Descargar texto si no existe ===\n",
    "file_path = \"shakespeare.txt\"\n",
    "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"â¬‡ï¸ Descargando texto de Shakespeare...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(f\"ðŸ“„ Archivo guardado como {file_path}\")\n",
    "\n",
    "# === Cargar texto ===\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    combined_text = f.read()\n",
    "\n",
    "print(f\"ðŸ“„ Texto cargado ({len(combined_text)} caracteres)\")\n",
    "\n",
    "# === Preprocesamiento ===\n",
    "def clean_text_for_gpt2(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\\"\\'\\(\\)]', '', text)\n",
    "    text = re.sub(r'\\s+([\\.,:;!?])', r'\\1', text)\n",
    "    return text.strip()\n",
    "\n",
    "clean_combined_text = clean_text_for_gpt2(combined_text)\n",
    "\n",
    "# === Tokenizador ===\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokens = tokenizer.encode(clean_combined_text)\n",
    "print(f\"ðŸ”  Tokens generados: {len(tokens)}\")\n",
    "\n",
    "# === Crear chunks ===\n",
    "def create_text_chunks(tokens, chunk_size=128, overlap=32):\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - chunk_size, chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if len(chunk) == chunk_size:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "CHUNK_SIZE = 128\n",
    "chunks = create_text_chunks(tokens, CHUNK_SIZE)\n",
    "print(f\"ðŸ“¦ Chunks creados: {len(chunks)}\")\n",
    "\n",
    "# === Clase Dataset personalizada ===\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, chunks, tokenizer, max_length=128):\n",
    "        self.chunks = chunks\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        if len(chunk) > self.max_length:\n",
    "            chunk = chunk[:self.max_length]\n",
    "        elif len(chunk) < self.max_length:\n",
    "            chunk += [self.tokenizer.pad_token_id] * (self.max_length - len(chunk))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(chunk, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor([1 if token != self.tokenizer.pad_token_id else 0 for token in chunk], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(chunk, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# === Crear Dataset y dividir ===\n",
    "if len(chunks) == 0:\n",
    "    raise ValueError(\"âš ï¸ No se generaron chunks. Aumenta el texto o reduce el tamaÃ±o de chunk.\")\n",
    "\n",
    "dataset = ShakespeareDataset(chunks, tokenizer, CHUNK_SIZE)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(f\"âœ… Dataset creado: {len(dataset)} ejemplos\")\n",
    "print(f\"   ðŸ”¹ Train: {len(train_dataset)}\")\n",
    "print(f\"   ðŸ”¸ Val: {len(val_dataset)}\")\n",
    "\n",
    "# === Mostrar un ejemplo ===\n",
    "example = dataset[0]\n",
    "print(\"\\nðŸ“Œ Ejemplo de entrada:\")\n",
    "print(f\"Input shape: {example['input_ids'].shape}\")\n",
    "print(\"Texto decodificado:\")\n",
    "print(tokenizer.decode(example['input_ids'][:50]))\n",
    "\n",
    "# === Guardar datos ===\n",
    "preprocessed_data = {\n",
    "    \"chunks\": chunks,\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"total_tokens\": len(tokens)\n",
    "}\n",
    "\n",
    "with open(\"preprocessed_shakespeare.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"\\nðŸ’¾ Datos preprocessados guardados en 'preprocessed_shakespeare.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "090a89e72dcd462291ba212c77c91b41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aedeaa5e902749c59f88acf9fdd54d36",
       "IPY_MODEL_6af00cf46bed4e19bfddb1d13ec9c9b5",
       "IPY_MODEL_640dbb9c803c41dfb4a6c44cf8de9efa"
      ],
      "layout": "IPY_MODEL_5f3935f30db84f4eae7a6bd9cf76abf2"
     }
    },
    "0ac8a6e43caf4e6596f1c03c428f615e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "114a935c3989491a9aca1263a6fa68c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "11f349bc5c0045bb94a702d61f176a07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1edce24cb707493fa2f69febb11c6444": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2101f4478de74d21b1d13600f2530b8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22ed64a40bee41828ac80666dffb7895": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca6f0b2f1fa4a6da44b73c0066a0a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70ebe4f77c0346d99a005ef448caf386",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bda0f6c908cc4ba1959ebcfccc29f618",
      "value": "â€‡665/665â€‡[00:00&lt;00:00,â€‡20.5kB/s]"
     }
    },
    "3328d4c6ba744920a412f60ec9190024": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ed9c42960e3411a910364857d73d3d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6345c42875c14f428d979f5d6c45b222",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98e0c4e7c6224f13bc87f884de436923",
      "value": 665
     }
    },
    "4363682f26a647bb8c1047f00ab6729a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1ad811d2ef419bbc1dbf78b647d40f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_11f349bc5c0045bb94a702d61f176a07",
      "value": "merges.txt:â€‡100%"
     }
    },
    "4a362d33e41b4cc0be3435f81488bef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5151520b211548e6bada1b8adca6181a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9aafc26f35794386a4208c866b2a1757",
       "IPY_MODEL_cdee77fb9e474375a1ab4e20789072a1",
       "IPY_MODEL_780f7b39dcab4395b89754a39d550f0f"
      ],
      "layout": "IPY_MODEL_ca4e16ff781a49d6afd55808c9ecc245"
     }
    },
    "576acb57204048429785ed022fc747b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f3935f30db84f4eae7a6bd9cf76abf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61f1aab215434dffaeb64d1c7d0632da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6345c42875c14f428d979f5d6c45b222": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "640dbb9c803c41dfb4a6c44cf8de9efa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd9624e2dcf04de89253512a87d75908",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_715e3f4a421b48729ebdf13094ed1f45",
      "value": "â€‡26.0/26.0â€‡[00:00&lt;00:00,â€‡439B/s]"
     }
    },
    "67cfbb63ff90404baef31a360cb3e241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "692f4311bc9f40faae6b3dfb828850c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6af00cf46bed4e19bfddb1d13ec9c9b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd3a4f11fc6f4db380207d8fc9ea74e3",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dcbae98fc69b4ff5afa41b758cfe6f94",
      "value": 26
     }
    },
    "6be7af639706460896c77088c6e0f8a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1edce24cb707493fa2f69febb11c6444",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2101f4478de74d21b1d13600f2530b8a",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "6d1ad811d2ef419bbc1dbf78b647d40f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6da9b477c5e646baa30238e5f059a155": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70ebe4f77c0346d99a005ef448caf386": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "715e3f4a421b48729ebdf13094ed1f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "780f7b39dcab4395b89754a39d550f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c62a7c0aa9c64381b01ada5ca3e17362",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ea4e95e55f6a4ca0a9dd722ce7852593",
      "value": "â€‡1.04M/1.04Mâ€‡[00:00&lt;00:00,â€‡6.52MB/s]"
     }
    },
    "88363a8a114a4f0f9b553854dac0e0d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a362d33e41b4cc0be3435f81488bef7",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_114a935c3989491a9aca1263a6fa68c9",
      "value": 456318
     }
    },
    "98e0c4e7c6224f13bc87f884de436923": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9aafc26f35794386a4208c866b2a1757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_692f4311bc9f40faae6b3dfb828850c2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fdc178cb5f84479a8ef2fec428beffd4",
      "value": "vocab.json:â€‡100%"
     }
    },
    "a6432e9a307942438e57338b5e0047db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6be7af639706460896c77088c6e0f8a9",
       "IPY_MODEL_dfe74e9a22ff4bfeb1ad47e4e713a2ef",
       "IPY_MODEL_d08d421b9b584cbdbd77d66406cdd6f6"
      ],
      "layout": "IPY_MODEL_6da9b477c5e646baa30238e5f059a155"
     }
    },
    "ac08b0e1d4b64ddbb4fc87abafb4a027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aece639a78d64761be478c2808f63d01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aedeaa5e902749c59f88acf9fdd54d36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8e862835f424d4eb8c41c835c9de1d0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_67cfbb63ff90404baef31a360cb3e241",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "bd3a4f11fc6f4db380207d8fc9ea74e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda0f6c908cc4ba1959ebcfccc29f618": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c62a7c0aa9c64381b01ada5ca3e17362": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9b59e54ab614684befc438ea007ac81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22ed64a40bee41828ac80666dffb7895",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0ac8a6e43caf4e6596f1c03c428f615e",
      "value": "â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡7.60MB/s]"
     }
    },
    "ca4e16ff781a49d6afd55808c9ecc245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd9624e2dcf04de89253512a87d75908": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdee77fb9e474375a1ab4e20789072a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aece639a78d64761be478c2808f63d01",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_576acb57204048429785ed022fc747b4",
      "value": 1042301
     }
    },
    "d08d421b9b584cbdbd77d66406cdd6f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8c600c418bc4845b2e189e345ec9812",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d124eab457ed4fd09a79ebb71a80cf34",
      "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡12.7MB/s]"
     }
    },
    "d124eab457ed4fd09a79ebb71a80cf34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d15d845f936a45df935b46e04c4cbfd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe6ab6b9b9a475fb9831a3270b68ca4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffa6d136227d4562a6008babe4a710b7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ac08b0e1d4b64ddbb4fc87abafb4a027",
      "value": "config.json:â€‡100%"
     }
    },
    "dcbae98fc69b4ff5afa41b758cfe6f94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dfe74e9a22ff4bfeb1ad47e4e713a2ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e108978b57fd4e43a31de2ef9fa6e695",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3328d4c6ba744920a412f60ec9190024",
      "value": 1355256
     }
    },
    "e108978b57fd4e43a31de2ef9fa6e695": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8c600c418bc4845b2e189e345ec9812": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8e862835f424d4eb8c41c835c9de1d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea4e95e55f6a4ca0a9dd722ce7852593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7d8839302a0464eb8292fe7e0768e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4363682f26a647bb8c1047f00ab6729a",
       "IPY_MODEL_88363a8a114a4f0f9b553854dac0e0d0",
       "IPY_MODEL_c9b59e54ab614684befc438ea007ac81"
      ],
      "layout": "IPY_MODEL_d15d845f936a45df935b46e04c4cbfd5"
     }
    },
    "f83bf28594d14387989ae0a9563ba464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbe6ab6b9b9a475fb9831a3270b68ca4",
       "IPY_MODEL_3ed9c42960e3411a910364857d73d3d1",
       "IPY_MODEL_2ca6f0b2f1fa4a6da44b73c0066a0a4f"
      ],
      "layout": "IPY_MODEL_61f1aab215434dffaeb64d1c7d0632da"
     }
    },
    "fdc178cb5f84479a8ef2fec428beffd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffa6d136227d4562a6008babe4a710b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
